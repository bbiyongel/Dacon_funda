{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:04:39.977365Z",
     "start_time": "2020-06-24T07:04:39.973365Z"
    }
   },
   "source": [
    "1.Folder\n",
    "    * './mart' 폴더에 (1)에서 저장한 Pickle 데이터가 들어 있어야 함\n",
    "    * './saved_models' 폴더가 생성되어 있어야 함\n",
    "2.Package\n",
    "    * Python3.6\n",
    "    * tensorflow == 1.5.0, keras == 2.2.2\n",
    "    * pandas, numpy, os, time, pickle, tensorflow, keras, matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import from Pickle\n",
    "* (1)에서 저장한 데이터 중, X_AE를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:38.494022Z",
     "start_time": "2020-06-26T06:55:36.531580Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os, time, pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "def write_pickle(data, path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def read_pickle(path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "mart_path = './mart'\n",
    "time_length = 44\n",
    "timelength = time_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:38.801091Z",
     "start_time": "2020-06-26T06:55:38.495022Z"
    }
   },
   "outputs": [],
   "source": [
    "X_AE = read_pickle(mart_path, '44_X_AE')\n",
    "feature_cols = read_pickle(mart_path, '44_feature_cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:38.805093Z",
     "start_time": "2020-06-26T06:55:38.802091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amount_43.0', 'amount_42.0', 'amount_41.0', 'amount_40.0', 'amount_39.0', 'amount_38.0', 'amount_37.0', 'amount_36.0', 'amount_35.0', 'amount_34.0', 'amount_33.0', 'amount_32.0', 'amount_31.0', 'amount_30.0', 'amount_29.0', 'amount_28.0', 'amount_27.0', 'amount_26.0', 'amount_25.0', 'amount_24.0', 'amount_23.0', 'amount_22.0', 'amount_21.0', 'amount_20.0', 'amount_19.0', 'amount_18.0', 'amount_17.0', 'amount_16.0', 'amount_15.0', 'amount_14.0', 'amount_13.0', 'amount_12.0', 'amount_11.0', 'amount_10.0', 'amount_9.0', 'amount_8.0', 'amount_7.0', 'amount_6.0', 'amount_5.0', 'amount_4.0', 'amount_3.0', 'amount_2.0', 'amount_1.0', 'amount_0']\n",
      "['installments_2', 'installments_3', 'installments_4', 'installments_5', 'installments_6', 'installments_7', 'installments_8', 'installments_9', 'installments_10', 'installments_12', 'installments_15', 'installments_18', 'installments_20', 'installments_22', 'installments_24', 'installments_36', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'ex-holyday', 'red_day', 'rest_day']\n"
     ]
    }
   ],
   "source": [
    "seq_cols = feature_cols[:time_length]\n",
    "print(seq_cols)\n",
    "\n",
    "meta_cols = feature_cols[time_length:]\n",
    "print(meta_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing for Sequnece Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:38.810092Z",
     "start_time": "2020-06-26T06:55:38.806092Z"
    }
   },
   "outputs": [],
   "source": [
    "def preproc_for_seq(array):\n",
    "    \n",
    "    array_seq = []\n",
    "    \n",
    "    # seq_cols\n",
    "    array_seq.append(array[:,:timelength].reshape((array.shape[0], timelength, 1)))\n",
    "    \n",
    "    # meta_cols\n",
    "    array_seq.append(array[:,timelength:])\n",
    "    \n",
    "    return array_seq\n",
    "\n",
    "X_AE_seq = preproc_for_seq(X_AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Represenation Learning - Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Architecture\n",
    "* ResNet Architecture를 사용\n",
    "* 본래 ResNet은 Image Data에 Conv2D를 사용하지만, 본 데이터는 Sequnece Data이므로 Conv1D를 사용\n",
    "* Sequence Data는 ResNet, Meta Data는 DNN\n",
    "* Concatenate 후, Reconstruction함 (Autoencoder). Loss Function은 mean_squared_error\n",
    "* Input -> Concatenate (Encoder)\n",
    "* Concatenate -> output (Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:38.817094Z",
     "start_time": "2020-06-26T06:55:38.811093Z"
    }
   },
   "outputs": [],
   "source": [
    "def res_unit(inputs, channels):\n",
    "    x = BatchNormalization()(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(channels, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(channels, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    added = Add()([inputs, x])\n",
    "    return added\n",
    "\n",
    "def res_unit_stride(inputs, channels):\n",
    "    x = BatchNormalization()(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(channels, kernel_size=3, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(channels, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    conv = Conv1D(channels, kernel_size=1, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    added = Add()([conv, x])\n",
    "    return added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:40.299428Z",
     "start_time": "2020-06-26T06:55:38.818094Z"
    }
   },
   "outputs": [],
   "source": [
    "# for seq_cols, Conv1D\n",
    "seq_in = Input(shape=(time_length,1))\n",
    "x = Conv1D(16, kernel_size=3, activation='relu', padding='same')(seq_in)\n",
    "x = MaxPooling1D(2, padding='same')(x)\n",
    "x =  res_unit(x, 16)\n",
    "x =  res_unit(x, 16)\n",
    "x =  res_unit(x, 16)\n",
    "x =  res_unit_stride(x, 32)\n",
    "x =  res_unit(x, 32)\n",
    "x =  res_unit(x, 32)\n",
    "x =  res_unit(x, 32)\n",
    "x =  res_unit_stride(x, 64)\n",
    "x =  res_unit(x, 64)\n",
    "x =  res_unit(x, 64)\n",
    "x =  res_unit(x, 64)\n",
    "x =  res_unit(x, 64)\n",
    "x =  res_unit_stride(x, 128)\n",
    "x =  res_unit(x, 128)\n",
    "x =  res_unit(x, 128)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "seq_out = x\n",
    "\n",
    "# for meta_cols\n",
    "meta_in = Input(shape=(len(meta_cols),))\n",
    "meta_out = Dense(20, kernel_initializer='he_normal', activation='elu')(meta_in)\n",
    "\n",
    "# Concat\n",
    "merges = Concatenate()([seq_out, meta_out])\n",
    "\n",
    "# decode\n",
    "seq_decode = Dense(time_length, kernel_initializer='he_normal')(merges)\n",
    "seq_decode = Reshape((time_length,1))(seq_decode)\n",
    "meta_decode = Dense(len(meta_cols), kernel_initializer='he_normal')(merges)\n",
    "\n",
    "model = Model(inputs=[seq_in, meta_in], outputs=[seq_decode, meta_decode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:40.310431Z",
     "start_time": "2020-06-26T06:55:40.300429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 44, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 44, 16)       64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 22, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 22, 16)       64          max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 22, 16)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 22, 16)       768         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 22, 16)       64          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 22, 16)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 22, 16)       768         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 22, 16)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 22, 16)       64          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 22, 16)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 22, 16)       768         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 22, 16)       64          conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 22, 16)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 22, 16)       768         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 22, 16)       0           add_1[0][0]                      \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 22, 16)       64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 22, 16)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 22, 16)       768         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 22, 16)       64          conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 22, 16)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 22, 16)       768         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 22, 16)       0           add_2[0][0]                      \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 22, 16)       64          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 22, 16)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 11, 32)       1536        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 11, 32)       128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 11, 32)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 11, 32)       512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 11, 32)       3072        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 11, 32)       0           conv1d_10[0][0]                  \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 11, 32)       128         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 11, 32)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 11, 32)       3072        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 11, 32)       128         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 11, 32)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 11, 32)       3072        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 11, 32)       0           add_4[0][0]                      \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 11, 32)       128         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 11, 32)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 11, 32)       3072        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 11, 32)       128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 11, 32)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 11, 32)       3072        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 11, 32)       0           add_5[0][0]                      \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 11, 32)       128         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 11, 32)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 11, 32)       3072        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 11, 32)       128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 11, 32)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 11, 32)       3072        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 11, 32)       0           add_6[0][0]                      \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 11, 32)       128         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 11, 32)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 6, 64)        6144        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 6, 64)        256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 6, 64)        0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 6, 64)        2048        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 6, 64)        12288       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 6, 64)        0           conv1d_19[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 6, 64)        256         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 6, 64)        0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 6, 64)        12288       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 6, 64)        256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 6, 64)        0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 6, 64)        12288       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 6, 64)        0           add_8[0][0]                      \n",
      "                                                                 conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 6, 64)        256         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 6, 64)        0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 6, 64)        12288       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 6, 64)        256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 6, 64)        0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 6, 64)        12288       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 6, 64)        0           add_9[0][0]                      \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 6, 64)        256         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 6, 64)        0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 6, 64)        12288       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 6, 64)        256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 6, 64)        0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 6, 64)        12288       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 6, 64)        0           add_10[0][0]                     \n",
      "                                                                 conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 6, 64)        256         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 6, 64)        0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 6, 64)        12288       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 6, 64)        256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 6, 64)        0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 6, 64)        12288       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 6, 64)        0           add_11[0][0]                     \n",
      "                                                                 conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 6, 64)        256         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 6, 64)        0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 3, 128)       24576       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 3, 128)       512         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 3, 128)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 3, 128)       8192        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 3, 128)       49152       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 3, 128)       0           conv1d_30[0][0]                  \n",
      "                                                                 conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 3, 128)       512         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 3, 128)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 3, 128)       49152       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 3, 128)       512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 3, 128)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 3, 128)       49152       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 3, 128)       0           add_13[0][0]                     \n",
      "                                                                 conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 3, 128)       512         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 3, 128)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 3, 128)       49152       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 3, 128)       512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 3, 128)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 3, 128)       49152       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 3, 128)       0           add_14[0][0]                     \n",
      "                                                                 conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 3, 128)       512         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 3, 128)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           520         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 148)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 44)           6556        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 44, 1)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 25)           3725        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 443,441\n",
      "Trainable params: 439,889\n",
      "Non-trainable params: 3,552\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:40.333436Z",
     "start_time": "2020-06-26T06:55:40.311431Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=['mean_squared_error', 'mean_squared_error'], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callback\n",
    "* Learning Rate를 잘 조절해준다는 Cyclic lr을 사용 (LRFinder Class)\n",
    "* epoch = 100, batch_size = 128, train_validation split 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:40.344439Z",
     "start_time": "2020-06-26T06:55:40.334436Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    '''\n",
    "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
    "    \n",
    "    # Usage\n",
    "        ```python\n",
    "            lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
    "            \n",
    "            lr_finder.plot_loss()\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
    "        \n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: https://arxiv.org/abs/1506.01186\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:40.348440Z",
     "start_time": "2020-06-26T06:55:40.345439Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T14:39:34.044182Z",
     "start_time": "2020-06-26T06:55:40.349440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "705725/705725 [==============================] - 300s 426us/step - loss: 0.4510 - reshape_1_loss: 0.3307 - dense_3_loss: 0.1203\n",
      "Epoch 2/100\n",
      "705725/705725 [==============================] - 293s 415us/step - loss: 0.1697 - reshape_1_loss: 0.1536 - dense_3_loss: 0.0161\n",
      "Epoch 3/100\n",
      "705725/705725 [==============================] - 301s 427us/step - loss: 0.1217 - reshape_1_loss: 0.1061 - dense_3_loss: 0.0156\n",
      "Epoch 4/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0958 - reshape_1_loss: 0.0803 - dense_3_loss: 0.0155\n",
      "Epoch 5/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0801 - reshape_1_loss: 0.0647 - dense_3_loss: 0.0154\n",
      "Epoch 6/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0707 - reshape_1_loss: 0.0554 - dense_3_loss: 0.0153\n",
      "Epoch 7/100\n",
      "705725/705725 [==============================] - 295s 417us/step - loss: 0.0632 - reshape_1_loss: 0.0479 - dense_3_loss: 0.0153\n",
      "Epoch 8/100\n",
      "705725/705725 [==============================] - 297s 421us/step - loss: 0.0578 - reshape_1_loss: 0.0425 - dense_3_loss: 0.0153\n",
      "Epoch 9/100\n",
      "705725/705725 [==============================] - 297s 421us/step - loss: 0.0532 - reshape_1_loss: 0.0380 - dense_3_loss: 0.0152\n",
      "Epoch 10/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0494 - reshape_1_loss: 0.0343 - dense_3_loss: 0.0151\n",
      "Epoch 11/100\n",
      "705725/705725 [==============================] - 290s 411us/step - loss: 0.0473 - reshape_1_loss: 0.0322 - dense_3_loss: 0.0151\n",
      "Epoch 12/100\n",
      "705725/705725 [==============================] - 290s 410us/step - loss: 0.0424 - reshape_1_loss: 0.0273 - dense_3_loss: 0.0151\n",
      "Epoch 13/100\n",
      "705725/705725 [==============================] - 287s 406us/step - loss: 0.0418 - reshape_1_loss: 0.0267 - dense_3_loss: 0.0151\n",
      "Epoch 14/100\n",
      "705725/705725 [==============================] - 296s 419us/step - loss: 0.0397 - reshape_1_loss: 0.0247 - dense_3_loss: 0.0150\n",
      "Epoch 15/100\n",
      "705725/705725 [==============================] - 297s 421us/step - loss: 0.0385 - reshape_1_loss: 0.0236 - dense_3_loss: 0.0150\n",
      "Epoch 16/100\n",
      "705725/705725 [==============================] - 289s 409us/step - loss: 0.0371 - reshape_1_loss: 0.0220 - dense_3_loss: 0.0151\n",
      "Epoch 17/100\n",
      "705725/705725 [==============================] - 288s 408us/step - loss: 0.0360 - reshape_1_loss: 0.0209 - dense_3_loss: 0.0151\n",
      "Epoch 18/100\n",
      "705725/705725 [==============================] - 289s 410us/step - loss: 0.0344 - reshape_1_loss: 0.0194 - dense_3_loss: 0.0149\n",
      "Epoch 19/100\n",
      "705725/705725 [==============================] - 291s 413us/step - loss: 0.0340 - reshape_1_loss: 0.0190 - dense_3_loss: 0.0150\n",
      "Epoch 20/100\n",
      "705725/705725 [==============================] - 290s 411us/step - loss: 0.0333 - reshape_1_loss: 0.0184 - dense_3_loss: 0.0149\n",
      "Epoch 21/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0325 - reshape_1_loss: 0.0176 - dense_3_loss: 0.0149\n",
      "Epoch 22/100\n",
      "705725/705725 [==============================] - 291s 412us/step - loss: 0.0313 - reshape_1_loss: 0.0164 - dense_3_loss: 0.0150\n",
      "Epoch 23/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0306 - reshape_1_loss: 0.0157 - dense_3_loss: 0.0149\n",
      "Epoch 24/100\n",
      "705725/705725 [==============================] - 290s 412us/step - loss: 0.0297 - reshape_1_loss: 0.0147 - dense_3_loss: 0.0150\n",
      "Epoch 25/100\n",
      "705725/705725 [==============================] - 291s 413us/step - loss: 0.0297 - reshape_1_loss: 0.0148 - dense_3_loss: 0.0149\n",
      "Epoch 26/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0291 - reshape_1_loss: 0.0141 - dense_3_loss: 0.0149\n",
      "Epoch 27/100\n",
      "705725/705725 [==============================] - 280s 396us/step - loss: 0.0282 - reshape_1_loss: 0.0133 - dense_3_loss: 0.0149\n",
      "Epoch 28/100\n",
      "705725/705725 [==============================] - 279s 395us/step - loss: 0.0280 - reshape_1_loss: 0.0130 - dense_3_loss: 0.0149\n",
      "Epoch 29/100\n",
      "705725/705725 [==============================] - 278s 394us/step - loss: 0.0277 - reshape_1_loss: 0.0128 - dense_3_loss: 0.0149\n",
      "Epoch 30/100\n",
      "705725/705725 [==============================] - 278s 393us/step - loss: 0.0270 - reshape_1_loss: 0.0121 - dense_3_loss: 0.0149\n",
      "Epoch 31/100\n",
      "705725/705725 [==============================] - 275s 390us/step - loss: 0.0270 - reshape_1_loss: 0.0121 - dense_3_loss: 0.0149\n",
      "Epoch 32/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0266 - reshape_1_loss: 0.0117 - dense_3_loss: 0.0149\n",
      "Epoch 33/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0267 - reshape_1_loss: 0.0118 - dense_3_loss: 0.0149\n",
      "Epoch 34/100\n",
      "705725/705725 [==============================] - 274s 388us/step - loss: 0.0261 - reshape_1_loss: 0.0112 - dense_3_loss: 0.0149\n",
      "Epoch 35/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0252 - reshape_1_loss: 0.0104 - dense_3_loss: 0.0148s - loss: 0.0253 - reshap\n",
      "Epoch 36/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0252 - reshape_1_loss: 0.0104 - dense_3_loss: 0.0148\n",
      "Epoch 37/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0252 - reshape_1_loss: 0.0103 - dense_3_loss: 0.0149\n",
      "Epoch 38/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0246 - reshape_1_loss: 0.0098 - dense_3_loss: 0.0148\n",
      "Epoch 39/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0248 - reshape_1_loss: 0.0099 - dense_3_loss: 0.0148\n",
      "Epoch 40/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0243 - reshape_1_loss: 0.0096 - dense_3_loss: 0.0147\n",
      "Epoch 41/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0243 - reshape_1_loss: 0.0095 - dense_3_loss: 0.0149\n",
      "Epoch 42/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0236 - reshape_1_loss: 0.0088 - dense_3_loss: 0.0148\n",
      "Epoch 43/100\n",
      "705725/705725 [==============================] - 274s 388us/step - loss: 0.0236 - reshape_1_loss: 0.0088 - dense_3_loss: 0.0148\n",
      "Epoch 44/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0241 - reshape_1_loss: 0.0093 - dense_3_loss: 0.0148\n",
      "Epoch 45/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0236 - reshape_1_loss: 0.0088 - dense_3_loss: 0.0148s - loss: 0.0236 - reshape_\n",
      "Epoch 46/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0233 - reshape_1_loss: 0.0086 - dense_3_loss: 0.0147\n",
      "Epoch 47/100\n",
      "705725/705725 [==============================] - 274s 388us/step - loss: 0.0233 - reshape_1_loss: 0.0086 - dense_3_loss: 0.0147\n",
      "Epoch 48/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0225 - reshape_1_loss: 0.0078 - dense_3_loss: 0.0147\n",
      "Epoch 49/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0230 - reshape_1_loss: 0.0082 - dense_3_loss: 0.0148s - loss: 0.0230 - reshape_\n",
      "Epoch 50/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0229 - reshape_1_loss: 0.0081 - dense_3_loss: 0.0147\n",
      "Epoch 51/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0224 - reshape_1_loss: 0.0077 - dense_3_loss: 0.0147\n",
      "Epoch 52/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0225 - reshape_1_loss: 0.0078 - dense_3_loss: 0.0147\n",
      "Epoch 53/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0221 - reshape_1_loss: 0.0075 - dense_3_loss: 0.0146\n",
      "Epoch 54/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0221 - reshape_1_loss: 0.0074 - dense_3_loss: 0.0147s - loss: 0.0\n",
      "Epoch 55/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0225 - reshape_1_loss: 0.0078 - dense_3_loss: 0.0147\n",
      "Epoch 56/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0219 - reshape_1_loss: 0.0072 - dense_3_loss: 0.0147\n",
      "Epoch 57/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0217 - reshape_1_loss: 0.0070 - dense_3_loss: 0.0147s - lo\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0215 - reshape_1_loss: 0.0069 - dense_3_loss: 0.0146\n",
      "Epoch 59/100\n",
      "705725/705725 [==============================] - 273s 386us/step - loss: 0.0218 - reshape_1_loss: 0.0072 - dense_3_loss: 0.0146\n",
      "Epoch 60/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0215 - reshape_1_loss: 0.0070 - dense_3_loss: 0.0145\n",
      "Epoch 61/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0209 - reshape_1_loss: 0.0063 - dense_3_loss: 0.0146\n",
      "Epoch 62/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0211 - reshape_1_loss: 0.0066 - dense_3_loss: 0.0145s - loss:\n",
      "Epoch 63/100\n",
      "705725/705725 [==============================] - 273s 386us/step - loss: 0.0209 - reshape_1_loss: 0.0065 - dense_3_loss: 0.0144\n",
      "Epoch 64/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0210 - reshape_1_loss: 0.0065 - dense_3_loss: 0.0145\n",
      "Epoch 65/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0208 - reshape_1_loss: 0.0063 - dense_3_loss: 0.0145\n",
      "Epoch 66/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0209 - reshape_1_loss: 0.0065 - dense_3_loss: 0.0144s - loss: 0.0209 - reshape_1_loss: 0.0065 - dense_3_\n",
      "Epoch 67/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0206 - reshape_1_loss: 0.0061 - dense_3_loss: 0.0144\n",
      "Epoch 68/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0205 - reshape_1_loss: 0.0061 - dense_3_loss: 0.0143\n",
      "Epoch 69/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0205 - reshape_1_loss: 0.0061 - dense_3_loss: 0.0144\n",
      "Epoch 70/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0204 - reshape_1_loss: 0.0061 - dense_3_loss: 0.0143\n",
      "Epoch 71/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0201 - reshape_1_loss: 0.0058 - dense_3_loss: 0.0143\n",
      "Epoch 72/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0201 - reshape_1_loss: 0.0058 - dense_3_loss: 0.0143s - loss: 0.0201 - reshape_1_loss: 0.0058\n",
      "Epoch 73/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0200 - reshape_1_loss: 0.0057 - dense_3_loss: 0.0143\n",
      "Epoch 74/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0201 - reshape_1_loss: 0.0058 - dense_3_loss: 0.0143\n",
      "Epoch 75/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0201 - reshape_1_loss: 0.0058 - dense_3_loss: 0.0143\n",
      "Epoch 76/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0198 - reshape_1_loss: 0.0055 - dense_3_loss: 0.0142\n",
      "Epoch 77/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0196 - reshape_1_loss: 0.0054 - dense_3_loss: 0.0142\n",
      "Epoch 78/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0198 - reshape_1_loss: 0.0057 - dense_3_loss: 0.0142\n",
      "Epoch 79/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0194 - reshape_1_loss: 0.0053 - dense_3_loss: 0.0142\n",
      "Epoch 80/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0194 - reshape_1_loss: 0.0053 - dense_3_loss: 0.0142s - loss: 0.0194 - reshape - ETA: \n",
      "Epoch 81/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0194 - reshape_1_loss: 0.0052 - dense_3_loss: 0.0142\n",
      "Epoch 82/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0193 - reshape_1_loss: 0.0051 - dense_3_loss: 0.0141\n",
      "Epoch 83/100\n",
      "705725/705725 [==============================] - 273s 386us/step - loss: 0.0196 - reshape_1_loss: 0.0054 - dense_3_loss: 0.0142\n",
      "Epoch 84/100\n",
      "705725/705725 [==============================] - 273s 386us/step - loss: 0.0193 - reshape_1_loss: 0.0051 - dense_3_loss: 0.0142\n",
      "Epoch 85/100\n",
      "705725/705725 [==============================] - 273s 386us/step - loss: 0.0191 - reshape_1_loss: 0.0051 - dense_3_loss: 0.0141\n",
      "Epoch 86/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0191 - reshape_1_loss: 0.0051 - dense_3_loss: 0.0140\n",
      "Epoch 87/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0190 - reshape_1_loss: 0.0049 - dense_3_loss: 0.0141\n",
      "Epoch 88/100\n",
      "705725/705725 [==============================] - 272s 386us/step - loss: 0.0191 - reshape_1_loss: 0.0050 - dense_3_loss: 0.0141\n",
      "Epoch 89/100\n",
      "705725/705725 [==============================] - 272s 386us/step - loss: 0.0191 - reshape_1_loss: 0.0051 - dense_3_loss: 0.0140\n",
      "Epoch 90/100\n",
      "705725/705725 [==============================] - 272s 386us/step - loss: 0.0188 - reshape_1_loss: 0.0048 - dense_3_loss: 0.0140\n",
      "Epoch 91/100\n",
      "705725/705725 [==============================] - 272s 386us/step - loss: 0.0188 - reshape_1_loss: 0.0047 - dense_3_loss: 0.0141\n",
      "Epoch 92/100\n",
      "705725/705725 [==============================] - 272s 386us/step - loss: 0.0191 - reshape_1_loss: 0.0051 - dense_3_loss: 0.0140\n",
      "Epoch 93/100\n",
      "705725/705725 [==============================] - 272s 386us/step - loss: 0.0186 - reshape_1_loss: 0.0046 - dense_3_loss: 0.0140\n",
      "Epoch 94/100\n",
      "705725/705725 [==============================] - 273s 386us/step - loss: 0.0187 - reshape_1_loss: 0.0046 - dense_3_loss: 0.0140\n",
      "Epoch 95/100\n",
      "705725/705725 [==============================] - 272s 386us/step - loss: 0.0184 - reshape_1_loss: 0.0044 - dense_3_loss: 0.0140\n",
      "Epoch 96/100\n",
      "705725/705725 [==============================] - 272s 386us/step - loss: 0.0184 - reshape_1_loss: 0.0045 - dense_3_loss: 0.0139\n",
      "Epoch 97/100\n",
      "705725/705725 [==============================] - 272s 386us/step - loss: 0.0188 - reshape_1_loss: 0.0048 - dense_3_loss: 0.0140\n",
      "Epoch 98/100\n",
      "705725/705725 [==============================] - 272s 386us/step - loss: 0.0183 - reshape_1_loss: 0.0042 - dense_3_loss: 0.0140\n",
      "Epoch 99/100\n",
      "705725/705725 [==============================] - 272s 386us/step - loss: 0.0185 - reshape_1_loss: 0.0045 - dense_3_loss: 0.0140s - loss: 0.0185 - reshape_1_loss: 0.0045 -\n",
      "Epoch 100/100\n",
      "705725/705725 [==============================] - 273s 387us/step - loss: 0.0186 - reshape_1_loss: 0.0046 - dense_3_loss: 0.0140\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_AE_seq, X_AE_seq,\n",
    "                    batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Save for Finetuing in Future\n",
    "* Modeling에서 Autoencoder의 중간층부터 학습에 사용하기 위해 저장한다.\n",
    "* encoder도 저장하고 model(=AE)도 저장하는데, 학습에는 encoder만 사용할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T14:39:34.059808Z",
     "start_time": "2020-06-26T14:39:34.044182Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_keras_model(model, filename):\n",
    "    model_json = model.to_json()\n",
    "    with open('{}.json'.format(filename), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights('{}.h5'.format(filename))\n",
    "\n",
    "def load_keras_model(filename):\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open('{}.json'.format(filename), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights('{}.h5'.format(filename))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:27.372574Z",
     "start_time": "2020-06-28T09:48:27.368582Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('./saved_models',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:31.989729Z",
     "start_time": "2020-06-28T09:48:28.026723Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = './saved_models'\n",
    "filename = '{}/ResNetAE'.format(model_dir)\n",
    "\n",
    "save_keras_model(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:48.387421Z",
     "start_time": "2020-06-28T09:48:48.379420Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = Model(inputs=[seq_in, meta_in], outputs=merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:48.481443Z",
     "start_time": "2020-06-28T09:48:48.389424Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = '{}/ResNetAE_encoder'.format(model_dir)\n",
    "\n",
    "save_keras_model(encoder, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Folder\n",
    "     * './mart' 폴더에 (1)에서 저장한 Pickle 데이터가 들어 있어야 함\n",
    "     * './saved_models' 폴더에 (2)에서 저장한 Keras Model 데이터가 들어 있어야 함\n",
    "     * './ckpt' 폴더가 생성되어 있어야 함\n",
    "2.Package\n",
    "     * Python3.6\n",
    "     * pandas, numpy, os, time, pickle, tensorflow, keras, matplotlib, sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 불러오기\n",
    "* (1)에서 저장한 데이터 중, (X, y)를 불러온다.\n",
    "* Model의 Generalization 성능을 측정하기 위해 Train / Validation data split을 0.85 : 0.15 비율로 나눈다.\n",
    "* preproc_for_seq 함수를 통해, Sequence data를 학습하기 위한 전처리를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:48.486445Z",
     "start_time": "2020-06-28T09:48:48.482443Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os, time, pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "def write_pickle(data, path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def read_pickle(path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "mart_path = './mart'\n",
    "time_length = 44\n",
    "timelength = time_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:48.850525Z",
     "start_time": "2020-06-28T09:48:48.487444Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = read_pickle(mart_path, '44_develop_data')\n",
    "feature_cols = read_pickle(mart_path, '44_feature_cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:48.854527Z",
     "start_time": "2020-06-28T09:48:48.851526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amount_43.0', 'amount_42.0', 'amount_41.0', 'amount_40.0', 'amount_39.0', 'amount_38.0', 'amount_37.0', 'amount_36.0', 'amount_35.0', 'amount_34.0', 'amount_33.0', 'amount_32.0', 'amount_31.0', 'amount_30.0', 'amount_29.0', 'amount_28.0', 'amount_27.0', 'amount_26.0', 'amount_25.0', 'amount_24.0', 'amount_23.0', 'amount_22.0', 'amount_21.0', 'amount_20.0', 'amount_19.0', 'amount_18.0', 'amount_17.0', 'amount_16.0', 'amount_15.0', 'amount_14.0', 'amount_13.0', 'amount_12.0', 'amount_11.0', 'amount_10.0', 'amount_9.0', 'amount_8.0', 'amount_7.0', 'amount_6.0', 'amount_5.0', 'amount_4.0', 'amount_3.0', 'amount_2.0', 'amount_1.0', 'amount_0']\n",
      "['installments_2', 'installments_3', 'installments_4', 'installments_5', 'installments_6', 'installments_7', 'installments_8', 'installments_9', 'installments_10', 'installments_12', 'installments_15', 'installments_18', 'installments_20', 'installments_22', 'installments_24', 'installments_36', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'ex-holyday', 'red_day', 'rest_day']\n"
     ]
    }
   ],
   "source": [
    "seq_cols = feature_cols[:time_length]\n",
    "print(seq_cols)\n",
    "\n",
    "meta_cols = feature_cols[time_length:]\n",
    "print(meta_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:49.245615Z",
     "start_time": "2020-06-28T09:48:48.855527Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:49.249617Z",
     "start_time": "2020-06-28T09:48:49.246615Z"
    }
   },
   "outputs": [],
   "source": [
    "def preproc_for_seq(array):\n",
    "    \n",
    "    array_seq = []\n",
    "    \n",
    "    # seq_cols\n",
    "    array_seq.append(array[:,:timelength].reshape((array.shape[0], timelength, 1)))\n",
    "    \n",
    "    # meta_cols\n",
    "    array_seq.append(array[:,timelength:])\n",
    "    \n",
    "    return array_seq\n",
    "\n",
    "X_seq = preproc_for_seq(X)\n",
    "y_seq = y\n",
    "\n",
    "X_train_seq = preproc_for_seq(X_train)\n",
    "X_valid_seq = preproc_for_seq(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling - ResNet & DNN FineTuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 불러오기\n",
    "* (2)에서 저장했던 Encoder를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:49.254617Z",
     "start_time": "2020-06-28T09:48:49.250616Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_keras_model(model, filename):\n",
    "    model_json = model.to_json()\n",
    "    with open('{}.json'.format(filename), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights('{}.h5'.format(filename))\n",
    "\n",
    "def load_keras_model(filename):\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open('{}.json'.format(filename), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights('{}.h5'.format(filename))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:52.632378Z",
     "start_time": "2020-06-28T09:48:49.255617Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = './saved_models'\n",
    "filename = '{}/ResNetAE_encoder'.format(model_dir)\n",
    "encoder = load_keras_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Architecture\n",
    "* Encoder의 Output 위에 Dense Layer를 하나만 쌓는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:52.644380Z",
     "start_time": "2020-06-28T09:48:52.633378Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_in, meta_in = encoder.inputs\n",
    "merges = encoder.outputs[0]\n",
    "y = Dense(1, kernel_initializer='he_normal', name='output')(merges)\n",
    "\n",
    "model = Model(inputs=[seq_in, meta_in], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:52.657383Z",
     "start_time": "2020-06-28T09:48:52.645380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 44, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 44, 16)       64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 22, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 22, 16)       64          max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 22, 16)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 22, 16)       768         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 22, 16)       64          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 22, 16)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 22, 16)       768         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 22, 16)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 22, 16)       64          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 22, 16)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 22, 16)       768         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 22, 16)       64          conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 22, 16)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 22, 16)       768         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 22, 16)       0           add_1[0][0]                      \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 22, 16)       64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 22, 16)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 22, 16)       768         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 22, 16)       64          conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 22, 16)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 22, 16)       768         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 22, 16)       0           add_2[0][0]                      \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 22, 16)       64          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 22, 16)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 11, 32)       1536        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 11, 32)       128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 11, 32)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 11, 32)       512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 11, 32)       3072        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 11, 32)       0           conv1d_10[0][0]                  \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 11, 32)       128         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 11, 32)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 11, 32)       3072        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 11, 32)       128         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 11, 32)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 11, 32)       3072        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 11, 32)       0           add_4[0][0]                      \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 11, 32)       128         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 11, 32)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 11, 32)       3072        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 11, 32)       128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 11, 32)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 11, 32)       3072        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 11, 32)       0           add_5[0][0]                      \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 11, 32)       128         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 11, 32)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 11, 32)       3072        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 11, 32)       128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 11, 32)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 11, 32)       3072        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 11, 32)       0           add_6[0][0]                      \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 11, 32)       128         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 11, 32)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 6, 64)        6144        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 6, 64)        256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 6, 64)        0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 6, 64)        2048        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 6, 64)        12288       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 6, 64)        0           conv1d_19[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 6, 64)        256         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 6, 64)        0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 6, 64)        12288       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 6, 64)        256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 6, 64)        0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 6, 64)        12288       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 6, 64)        0           add_8[0][0]                      \n",
      "                                                                 conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 6, 64)        256         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 6, 64)        0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 6, 64)        12288       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 6, 64)        256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 6, 64)        0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 6, 64)        12288       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 6, 64)        0           add_9[0][0]                      \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 6, 64)        256         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 6, 64)        0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 6, 64)        12288       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 6, 64)        256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 6, 64)        0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 6, 64)        12288       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 6, 64)        0           add_10[0][0]                     \n",
      "                                                                 conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 6, 64)        256         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 6, 64)        0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 6, 64)        12288       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 6, 64)        256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 6, 64)        0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 6, 64)        12288       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 6, 64)        0           add_11[0][0]                     \n",
      "                                                                 conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 6, 64)        256         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 6, 64)        0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 3, 128)       24576       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 3, 128)       512         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 3, 128)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 3, 128)       8192        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 3, 128)       49152       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 3, 128)       0           conv1d_30[0][0]                  \n",
      "                                                                 conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 3, 128)       512         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 3, 128)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 3, 128)       49152       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 3, 128)       512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 3, 128)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 3, 128)       49152       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 3, 128)       0           add_13[0][0]                     \n",
      "                                                                 conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 3, 128)       512         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 3, 128)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 3, 128)       49152       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 3, 128)       512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 3, 128)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 3, 128)       49152       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 3, 128)       0           add_14[0][0]                     \n",
      "                                                                 conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 3, 128)       512         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 3, 128)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           520         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 148)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            149         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 433,309\n",
      "Trainable params: 429,757\n",
      "Non-trainable params: 3,552\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss & Optimizer\n",
    "* Loss는 Competition 주최측에서 공지한 Evaluation Function * (-1)을 사용한다.\n",
    "* custom_loss가 그 역할을 수행한다.\n",
    "* optimizer는 Adam을 사용하고 Learning Rate는 0.001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:52.661385Z",
     "start_time": "2020-06-28T09:48:52.658384Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    default_position = tf.less(y_true, y_pred)\n",
    "    profit_position = tf.logical_not(default_position)\n",
    "    pos = tf.reduce_sum(y_pred*tf.cast(profit_position, tf.float32)*13/365)\n",
    "    neg = tf.reduce_sum((y_true-y_pred)*tf.cast(default_position, tf.float32))\n",
    "    loss = -pos-neg\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:52.685390Z",
     "start_time": "2020-06-28T09:48:52.662384Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=custom_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks\n",
    "* Learning Rate를 잘 조절해준다는 Cyclic lr을 사용 (LRFinder Class)\n",
    "* epoch = 100, batch_size = 128\n",
    "* ModelCheckpoint Callback을 사용해서, Validation data에서 역대 최저 Loss를 갱신할 때마다, 모델을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T09:48:52.696392Z",
     "start_time": "2020-06-28T09:48:52.688390Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    '''\n",
    "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
    "    \n",
    "    # Usage\n",
    "        ```python\n",
    "            lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
    "            \n",
    "            lr_finder.plot_loss()\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
    "        \n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: https://arxiv.org/abs/1506.01186\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T10:42:38.278318Z",
     "start_time": "2020-06-28T10:42:38.273316Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 128\n",
    "epoch_size = len(X_train)\n",
    "\n",
    "lr_finder = LRFinder(min_lr=1e-5, \n",
    "                     max_lr=1e-3, \n",
    "                     steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                     epochs=epochs)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
    "\n",
    "ckpt_dir = './ckpt'\n",
    "os.makedirs(ckpt_dir,exist_ok=True)\n",
    "ckpt_path = ckpt_dir + '/ResNetFinetuning_{epoch:02d}_valloss{val_loss:.2f}.hdf5'\n",
    "ckpt = keras.callbacks.ModelCheckpoint(ckpt_path, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "#callbacks = [lr_finder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T00:34:05.692512Z",
     "start_time": "2020-06-28T17:28:22.578019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 569907 samples, validate on 100572 samples\n",
      "Epoch 1/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.1212 - val_loss: 1.5206\n",
      "Epoch 2/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.1202 - val_loss: 1.6764\n",
      "Epoch 3/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.1209 - val_loss: 1.1902\n",
      "Epoch 4/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.1141 - val_loss: 1.4956\n",
      "Epoch 5/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.1147 - val_loss: 1.2472\n",
      "Epoch 6/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.1143 - val_loss: 1.7398\n",
      "Epoch 7/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.1141 - val_loss: 1.3711\n",
      "Epoch 8/100\n",
      "569907/569907 [==============================] - 257s 451us/step - loss: 1.1112 - val_loss: 1.4620\n",
      "Epoch 9/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.1106 - val_loss: 1.3306\n",
      "Epoch 10/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.1052 - val_loss: 1.1366\n",
      "Epoch 11/100\n",
      "569907/569907 [==============================] - 256s 450us/step - loss: 1.1089 - val_loss: 1.2734\n",
      "Epoch 12/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.1070 - val_loss: 1.2885\n",
      "Epoch 13/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.1029 - val_loss: 1.4403\n",
      "Epoch 14/100\n",
      "569907/569907 [==============================] - 257s 450us/step - loss: 1.1011 - val_loss: 1.5033\n",
      "Epoch 15/100\n",
      "569907/569907 [==============================] - 256s 450us/step - loss: 1.0985 - val_loss: 1.1952\n",
      "Epoch 16/100\n",
      "569907/569907 [==============================] - 257s 450us/step - loss: 1.0970 - val_loss: 1.3321\n",
      "Epoch 17/100\n",
      "569907/569907 [==============================] - 256s 448us/step - loss: 1.0920 - val_loss: 1.2749\n",
      "Epoch 18/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0954 - val_loss: 1.5426\n",
      "Epoch 19/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0973 - val_loss: 1.2726\n",
      "Epoch 20/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0897 - val_loss: 1.4604\n",
      "Epoch 21/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0882 - val_loss: 1.6078\n",
      "Epoch 22/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0883 - val_loss: 1.2526\n",
      "Epoch 23/100\n",
      "569907/569907 [==============================] - 257s 451us/step - loss: 1.0892 - val_loss: 1.4840\n",
      "Epoch 24/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0913 - val_loss: 1.3379\n",
      "Epoch 25/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0859 - val_loss: 1.6621\n",
      "Epoch 26/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0850 - val_loss: 1.9140\n",
      "Epoch 27/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0854 - val_loss: 1.5315\n",
      "Epoch 28/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0828 - val_loss: 1.6984\n",
      "Epoch 29/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0835 - val_loss: 1.6049\n",
      "Epoch 30/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0806 - val_loss: 1.2499\n",
      "Epoch 31/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0791 - val_loss: 1.6991\n",
      "Epoch 32/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0782 - val_loss: 1.2809\n",
      "Epoch 33/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0763 - val_loss: 1.6417\n",
      "Epoch 34/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0780 - val_loss: 1.1807\n",
      "Epoch 35/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0767 - val_loss: 1.4584\n",
      "Epoch 36/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0745 - val_loss: 1.5395\n",
      "Epoch 37/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0714 - val_loss: 1.6897\n",
      "Epoch 38/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0725 - val_loss: 1.2732\n",
      "Epoch 39/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0706 - val_loss: 2.2028\n",
      "Epoch 40/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0678 - val_loss: 1.1812\n",
      "Epoch 41/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0686 - val_loss: 1.0472\n",
      "Epoch 42/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0675 - val_loss: 1.3400\n",
      "Epoch 43/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0639 - val_loss: 1.5035\n",
      "Epoch 44/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0650 - val_loss: 1.3334\n",
      "Epoch 45/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0625 - val_loss: 1.1113\n",
      "Epoch 46/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0611 - val_loss: 1.4845\n",
      "Epoch 47/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0602 - val_loss: 2.6835\n",
      "Epoch 48/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0636 - val_loss: 2.0729\n",
      "Epoch 49/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0620 - val_loss: 1.3878\n",
      "Epoch 50/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0557 - val_loss: 1.2613\n",
      "Epoch 51/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0595 - val_loss: 1.2713\n",
      "Epoch 52/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0618 - val_loss: 1.2192\n",
      "Epoch 53/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0533 - val_loss: 1.3329\n",
      "Epoch 54/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0535 - val_loss: 1.0981\n",
      "Epoch 55/100\n",
      "569907/569907 [==============================] - 256s 448us/step - loss: 1.0525 - val_loss: 1.4742\n",
      "Epoch 56/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0562 - val_loss: 1.5514\n",
      "Epoch 57/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0528 - val_loss: 1.7954\n",
      "Epoch 58/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0477 - val_loss: 1.3838\n",
      "Epoch 59/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0513 - val_loss: 1.2339\n",
      "Epoch 60/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0517 - val_loss: 1.4309\n",
      "Epoch 61/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0502 - val_loss: 1.3688\n",
      "Epoch 62/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0462 - val_loss: 1.1597\n",
      "Epoch 63/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0423 - val_loss: 1.2195\n",
      "Epoch 64/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0456 - val_loss: 1.5001\n",
      "Epoch 65/100\n",
      "569907/569907 [==============================] - 256s 448us/step - loss: 1.0496 - val_loss: 1.1242\n",
      "Epoch 66/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0429 - val_loss: 1.4914\n",
      "Epoch 67/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0391 - val_loss: 1.4535\n",
      "Epoch 68/100\n",
      "569907/569907 [==============================] - 256s 448us/step - loss: 1.0388 - val_loss: 2.0934\n",
      "Epoch 69/100\n",
      "569907/569907 [==============================] - 256s 448us/step - loss: 1.0438 - val_loss: 1.0896\n",
      "Epoch 70/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0421 - val_loss: 1.1884\n",
      "Epoch 71/100\n",
      "569907/569907 [==============================] - 256s 448us/step - loss: 1.0375 - val_loss: 1.6545\n",
      "Epoch 72/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0376 - val_loss: 1.2296\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0375 - val_loss: 1.1122\n",
      "Epoch 74/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0344 - val_loss: 1.4759\n",
      "Epoch 75/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0343 - val_loss: 1.8965\n",
      "Epoch 76/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0334 - val_loss: 1.1086\n",
      "Epoch 77/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0359 - val_loss: 1.4247\n",
      "Epoch 78/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0285 - val_loss: 1.3452\n",
      "Epoch 79/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0306 - val_loss: 1.4869\n",
      "Epoch 80/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0332 - val_loss: 1.3762\n",
      "Epoch 81/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0299 - val_loss: 1.3506\n",
      "Epoch 82/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0287 - val_loss: 1.6272\n",
      "Epoch 83/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0257 - val_loss: 1.3313\n",
      "Epoch 84/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0296 - val_loss: 1.4398\n",
      "Epoch 85/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0239 - val_loss: 1.4114\n",
      "Epoch 86/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0266 - val_loss: 1.6229\n",
      "Epoch 87/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0216 - val_loss: 1.1793\n",
      "Epoch 88/100\n",
      "569907/569907 [==============================] - 256s 450us/step - loss: 1.0261 - val_loss: 1.9049\n",
      "Epoch 89/100\n",
      "569907/569907 [==============================] - 256s 450us/step - loss: 1.0211 - val_loss: 1.3525\n",
      "Epoch 90/100\n",
      "569907/569907 [==============================] - 256s 450us/step - loss: 1.0206 - val_loss: 1.5140\n",
      "Epoch 91/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0233 - val_loss: 1.8450\n",
      "Epoch 92/100\n",
      "569907/569907 [==============================] - 256s 449us/step - loss: 1.0185 - val_loss: 1.1488\n",
      "Epoch 93/100\n",
      "569907/569907 [==============================] - 259s 454us/step - loss: 1.0212 - val_loss: 1.1086\n",
      "Epoch 94/100\n",
      "569907/569907 [==============================] - 257s 451us/step - loss: 1.0148 - val_loss: 1.3346\n",
      "Epoch 95/100\n",
      "569907/569907 [==============================] - 249s 437us/step - loss: 1.0205 - val_loss: 1.1278\n",
      "Epoch 96/100\n",
      "569907/569907 [==============================] - 252s 441us/step - loss: 1.0088 - val_loss: 1.5521\n",
      "Epoch 97/100\n",
      "569907/569907 [==============================] - 250s 438us/step - loss: 1.0172 - val_loss: 1.1986\n",
      "Epoch 98/100\n",
      "569907/569907 [==============================] - 255s 448us/step - loss: 1.0117 - val_loss: 1.1818\n",
      "Epoch 99/100\n",
      "569907/569907 [==============================] - 258s 452us/step - loss: 1.0125 - val_loss: 1.3222\n",
      "Epoch 100/100\n",
      "569907/569907 [==============================] - 255s 447us/step - loss: 1.0059 - val_loss: 1.1736\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_seq, y_train,\n",
    "                    batch_size=batch_size, epochs=epochs,\n",
    "                    callbacks=[ckpt, lr_finder],\n",
    "                    validation_data=(X_valid_seq, y_valid), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Loading\n",
    "* 위 Training Log를 봤을 때, 국소적 역대 최고 Validation Loss를 기록한 것은, epoch 37, epoch 76, epoch 91, epoch 97 네 개이다\n",
    "* 아주 역대 최고는 epoch 97이지만, 76 ~ 91, 36 ~76 사이에 Validation Loss 하락 공백이 두 번 있었기 때문에, Overfitting을 의심하지 않을 수 없다.\n",
    "* 결론적으로, Competition에 Submit을 했을 때, 아래와 같은 Competition 점수를 얻었다\n",
    "    * epoch 37 = 53.8752점\n",
    "    * epoch 76 = 52.48691점 (3등 달성)\n",
    "    * epoch 91 = 66.65879점\n",
    "    * epoch 97 = 92점\n",
    "* 따라서, epoch 76에서의 ResNet + DNN Finetuning 모델은 training Loss 1.15, validation Loss 1.07, Test Score 52.48691의 우수한 성적을 내는 Overfitting이 되지 않으면서 우수한 Generalization 성능을 내는 우수한 단일 모델이라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T05:38:32.124471Z",
     "start_time": "2020-06-29T05:38:30.971211Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights(ckpt_dir +'/ResNetFinetuning_41_valloss1.05.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T05:38:32.622583Z",
     "start_time": "2020-06-29T05:38:32.617583Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_keras_model(model, filename):\n",
    "    model_json = model.to_json()\n",
    "    with open('{}.json'.format(filename), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights('{}.h5'.format(filename))\n",
    "\n",
    "def load_keras_model(filename):\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open('{}.json'.format(filename), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights('{}.h5'.format(filename))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T05:38:33.382755Z",
     "start_time": "2020-06-29T05:38:33.272739Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = './saved_models'\n",
    "filename = '{}/ResNetFinetuning'.format(model_dir)\n",
    "\n",
    "save_keras_model(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data, Validation Data에 대해 y를 Predict한다.\n",
    "* Test Data: Submit을 위해\n",
    "* Validation Data: Stacking 등을 위해 (하지 않았지만)\n",
    "* y는 Scaling이 되어 있는 상태이므로, inverse_transform을 수행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T05:38:58.511455Z",
     "start_time": "2020-06-29T05:38:34.761075Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = read_pickle(mart_path, '{}_X_test'.format(timelength))\n",
    "y_scaler = read_pickle(mart_path, '{}_y_scaler'.format(timelength))\n",
    "\n",
    "X_test_seq = preproc_for_seq(X_test)\n",
    "\n",
    "y_pred = model.predict(X_test_seq)\n",
    "y_pred_valid = model.predict(X_valid_seq)\n",
    "\n",
    "y_pred = y_scaler.inverse_transform(y_pred)\n",
    "y_pred_valid = y_scaler.inverse_transform(y_pred_valid)\n",
    "y_true_valid = y_scaler.inverse_transform(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data에 대한 Prediction을 Submission 양식에 맞추어 만들어낸다.\n",
    "* (1) Wranling 파트에서 store_id를 중복을 피하기 위해 조작을 했으므로, 본래의 store_id를 복구하기 위해 test_data를 다시 불러와서 원래의 store_id 정보를 얻어낸다.\n",
    "* Test data에 대한 Prediction과 함께 조합하여, Submission 파일을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T05:39:00.953713Z",
     "start_time": "2020-06-29T05:38:58.512456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Meta 정보 지정\n",
    "data_path = \"./data/\"\n",
    "train_filename = \"train.csv\"\n",
    "test_filename = \"test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(data_path + train_filename)\n",
    "test_data = pd.read_csv(data_path + test_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T05:39:00.974719Z",
     "start_time": "2020-06-29T05:39:00.954714Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'store_id': test_data['store_id'].unique(), 'total_sales': y_pred.reshape(len(y_pred))})\n",
    "submission.to_csv('submission_ResNetFinetuning76epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data에 대한 Prediction을 추후 Stacking / Blending 등을 위해 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T05:39:00.981720Z",
     "start_time": "2020-06-29T05:39:00.975719Z"
    }
   },
   "outputs": [],
   "source": [
    "write_pickle(y_pred_valid, \".\", 'y_pred_valid_ResNetFinetuning76epoch')\n",
    "write_pickle(y_true_valid, \".\", 'y_true_valid_ResNetFinetuning76epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "* 대회 주최측에서 알려준 Evaluation metric을 Python Function으로 만들었다.\n",
    "* Validation에 Evaluation Function을 적용했을 때, 101732063을 얻었고, 이는 만점 대비 0.4692700349674595%의 성과이다.\n",
    "* 이는 내가 지금까지 Evaluation했을 때 결과 중 Top 5 안에 들면서, Submit했을 때도 높은 점수를 얻은 결과라 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T05:49:17.814078Z",
     "start_time": "2020-06-29T05:49:17.810077Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred, verbose=True):\n",
    "    \n",
    "    assert len(y_true) == len(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(len(y_true),)\n",
    "    y_pred = y_pred.reshape(len(y_true),)\n",
    "    \n",
    "    default_position = y_true - y_pred < 0\n",
    "    profit_position = ~default_position\n",
    "    pos = np.sum(y_pred[profit_position]*13/365)\n",
    "    neg = np.sum(y_true[default_position] - y_pred[default_position])\n",
    "    score = pos + neg\n",
    "    if verbose:\n",
    "        print(\"The positive score: \")\n",
    "        print(pos)\n",
    "        print(\"The negative score: \")\n",
    "        print(neg)\n",
    "        print(\"The total score: \")\n",
    "        print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T05:49:18.287184Z",
     "start_time": "2020-06-29T05:49:18.282183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The positive score: \n",
      "127989500.0\n",
      "The negative score: \n",
      "-18974502.745571494\n",
      "The total score: \n",
      "109015001.2544285\n"
     ]
    }
   ],
   "source": [
    "score = get_score(y_true_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T05:49:24.532599Z",
     "start_time": "2020-06-29T05:49:24.528598Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_percentage_to_upperbd(y_true, y_pred, verbose=False):\n",
    "    upperbd_score = get_score(y_true, y_true, verbose=False)\n",
    "    score = get_score(y_true, y_pred, verbose=False)\n",
    "    percentage = score/upperbd_score\n",
    "    if verbose:\n",
    "        print(\"The upper bound score: \")\n",
    "        print(upperbd_score)\n",
    "        print(\"The obtained score: \")\n",
    "        print(score)\n",
    "        print(\"The percentage to upper bound: \")\n",
    "        print(percentage)\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T05:49:25.381782Z",
     "start_time": "2020-06-29T05:49:25.376781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upper bound score: \n",
      "216787895.46575347\n",
      "The obtained score: \n",
      "109015001.2544285\n",
      "The percentage to upper bound: \n",
      "0.5028647979640075\n"
     ]
    }
   ],
   "source": [
    "percentage = get_percentage_to_upperbd(y_true_valid, y_pred_valid, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
