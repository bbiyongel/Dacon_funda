{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:04:39.977365Z",
     "start_time": "2020-06-24T07:04:39.973365Z"
    }
   },
   "source": [
    "1.Folder\n",
    "    * './mart' 폴더에 (1)에서 저장한 Pickle 데이터가 들어 있어야 함\n",
    "    * './saved_models' 폴더가 생성되어 있어야 함\n",
    "2.Package\n",
    "    * Python3.6\n",
    "    * pandas, numpy, os, time, pickle, tensorflow, keras, matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import from Pickle\n",
    "* (1)에서 저장한 데이터 중, X_AE를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:04:54.272816Z",
     "start_time": "2020-06-24T07:04:54.183795Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-83555830bb28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGRU\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os, time, pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "def write_pickle(data, path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def read_pickle(path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "mart_path = './mart'\n",
    "time_length = 44\n",
    "timelength = time_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AE = read_pickle(mart_path, '44_X_AE')\n",
    "feature_cols = read_pickle(mart_path, '44_feature_cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_cols = feature_cols[:time_length]\n",
    "print(seq_cols)\n",
    "\n",
    "meta_cols = feature_cols[time_length:]\n",
    "print(meta_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing for Sequnece Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_for_seq(array):\n",
    "    \n",
    "    array_seq = []\n",
    "    \n",
    "    # seq_cols\n",
    "    array_seq.append(array[:,:timelength].reshape((array.shape[0], timelength, 1)))\n",
    "    \n",
    "    # meta_cols\n",
    "    array_seq.append(array[:,timelength:])\n",
    "    \n",
    "    return array_seq\n",
    "\n",
    "X_AE_seq = preproc_for_seq(X_AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Represenation Learning - Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Architecture\n",
    "* ResNet Architecture를 사용\n",
    "* 본래 ResNet은 Image Data에 Conv2D를 사용하지만, 본 데이터는 Sequnece Data이므로 Conv1D를 사용\n",
    "* Sequence Data는 ResNet, Meta Data는 DNN\n",
    "* Concatenate 후, Reconstruction함 (Autoencoder). Loss Function은 mean_squared_error\n",
    "* Input -> Concatenate (Encoder)\n",
    "* Concatenate -> output (Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_unit(inputs, channels):\n",
    "    x = BatchNormalization()(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(channels, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(channels, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    added = Add()([inputs, x])\n",
    "    return added\n",
    "\n",
    "def res_unit_stride(inputs, channels):\n",
    "    x = BatchNormalization()(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(channels, kernel_size=3, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(channels, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    conv = Conv1D(channels, kernel_size=1, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    added = Add()([conv, x])\n",
    "    return added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seq_cols, Conv1D\n",
    "seq_in = Input(shape=(time_length,1))\n",
    "x = Conv1D(16, kernel_size=3, activation='relu', padding='same')(seq_in)\n",
    "x = MaxPooling1D(2, padding='same')(x)\n",
    "x =  res_unit(x, 16)\n",
    "x =  res_unit(x, 16)\n",
    "x =  res_unit(x, 16)\n",
    "x =  res_unit_stride(x, 32)\n",
    "x =  res_unit(x, 32)\n",
    "x =  res_unit(x, 32)\n",
    "x =  res_unit(x, 32)\n",
    "x =  res_unit_stride(x, 64)\n",
    "x =  res_unit(x, 64)\n",
    "x =  res_unit(x, 64)\n",
    "x =  res_unit(x, 64)\n",
    "x =  res_unit(x, 64)\n",
    "x =  res_unit_stride(x, 128)\n",
    "x =  res_unit(x, 128)\n",
    "x =  res_unit(x, 128)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "seq_out = x\n",
    "\n",
    "# for meta_cols\n",
    "meta_in = Input(shape=(len(meta_cols),))\n",
    "meta_out = Dense(20, kernel_initializer='he_normal', activation='elu')(meta_in)\n",
    "\n",
    "# Concat\n",
    "merges = Concatenate()([seq_out, meta_out])\n",
    "\n",
    "# decode\n",
    "seq_decode = Dense(time_length, kernel_initializer='he_normal')(merges)\n",
    "seq_decode = Reshape((time_length,1))(seq_decode)\n",
    "meta_decode = Dense(len(meta_cols), kernel_initializer='he_normal')(merges)\n",
    "\n",
    "model = Model(inputs=[seq_in, meta_in], outputs=[seq_decode, meta_decode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=['mean_squared_error', 'mean_squared_error'], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callback\n",
    "* Learning Rate를 잘 조절해준다는 Cyclic lr을 사용 (LRFinder Class)\n",
    "* epoch = 100, batch_size = 128, train_validation split 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    '''\n",
    "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
    "    \n",
    "    # Usage\n",
    "        ```python\n",
    "            lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
    "            \n",
    "            lr_finder.plot_loss()\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
    "        \n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: https://arxiv.org/abs/1506.01186\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_AE_seq, X_AE_seq,\n",
    "                    batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Save for Finetuing in Future\n",
    "* Modeling에서 Autoencoder의 중간층부터 학습에 사용하기 위해 저장한다.\n",
    "* encoder도 저장하고 model(=AE)도 저장하는데, 학습에는 encoder만 사용할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keras_model(model, filename):\n",
    "    model_json = model.to_json()\n",
    "    with open('{}.json'.format(filename), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights('{}.h5'.format(filename))\n",
    "\n",
    "def load_keras_model(filename):\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open('{}.json'.format(filename), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights('{}.h5'.format(filename))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './saved_models'\n",
    "filename = '{}/ResNetAE'.format(model_dir)\n",
    "\n",
    "save_keras_model(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=[seq_in, meta_in], outputs=merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '{}/ResNetAE_encoder'.format(model_dir)\n",
    "\n",
    "save_keras_model(encoder, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
