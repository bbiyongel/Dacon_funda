{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:04:39.977365Z",
     "start_time": "2020-06-24T07:04:39.973365Z"
    }
   },
   "source": [
    "1.Folder\n",
    "    * './mart' 폴더에 (1)에서 저장한 Pickle 데이터가 들어 있어야 함\n",
    "    * './saved_models' 폴더가 생성되어 있어야 함\n",
    "2.Package\n",
    "    * Python3.6\n",
    "    * tensorflow == 1.5.0, keras == 2.2.2\n",
    "    * pandas, numpy, os, time, pickle, tensorflow, keras, matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import from Pickle\n",
    "* (1)에서 저장한 데이터 중, X_AE를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:38.494022Z",
     "start_time": "2020-06-26T06:55:36.531580Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os, time, pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "def write_pickle(data, path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def read_pickle(path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "mart_path = './mart'\n",
    "time_length = 44\n",
    "timelength = time_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:38.801091Z",
     "start_time": "2020-06-26T06:55:38.495022Z"
    }
   },
   "outputs": [],
   "source": [
    "X_AE = read_pickle(mart_path, '44_X_AE')\n",
    "feature_cols = read_pickle(mart_path, '44_feature_cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:38.805093Z",
     "start_time": "2020-06-26T06:55:38.802091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amount_43.0', 'amount_42.0', 'amount_41.0', 'amount_40.0', 'amount_39.0', 'amount_38.0', 'amount_37.0', 'amount_36.0', 'amount_35.0', 'amount_34.0', 'amount_33.0', 'amount_32.0', 'amount_31.0', 'amount_30.0', 'amount_29.0', 'amount_28.0', 'amount_27.0', 'amount_26.0', 'amount_25.0', 'amount_24.0', 'amount_23.0', 'amount_22.0', 'amount_21.0', 'amount_20.0', 'amount_19.0', 'amount_18.0', 'amount_17.0', 'amount_16.0', 'amount_15.0', 'amount_14.0', 'amount_13.0', 'amount_12.0', 'amount_11.0', 'amount_10.0', 'amount_9.0', 'amount_8.0', 'amount_7.0', 'amount_6.0', 'amount_5.0', 'amount_4.0', 'amount_3.0', 'amount_2.0', 'amount_1.0', 'amount_0']\n",
      "['installments_2', 'installments_3', 'installments_4', 'installments_5', 'installments_6', 'installments_7', 'installments_8', 'installments_9', 'installments_10', 'installments_12', 'installments_15', 'installments_18', 'installments_20', 'installments_22', 'installments_24', 'installments_36', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'ex-holyday', 'red_day', 'rest_day']\n"
     ]
    }
   ],
   "source": [
    "seq_cols = feature_cols[:time_length]\n",
    "print(seq_cols)\n",
    "\n",
    "meta_cols = feature_cols[time_length:]\n",
    "print(meta_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing for Sequnece Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:38.810092Z",
     "start_time": "2020-06-26T06:55:38.806092Z"
    }
   },
   "outputs": [],
   "source": [
    "def preproc_for_seq(array):\n",
    "    \n",
    "    array_seq = []\n",
    "    \n",
    "    # seq_cols\n",
    "    array_seq.append(array[:,:timelength].reshape((array.shape[0], timelength, 1)))\n",
    "    \n",
    "    # meta_cols\n",
    "    array_seq.append(array[:,timelength:])\n",
    "    \n",
    "    return array_seq\n",
    "\n",
    "X_AE_seq = preproc_for_seq(X_AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Represenation Learning - Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Architecture\n",
    "* ResNet Architecture를 사용\n",
    "* 본래 ResNet은 Image Data에 Conv2D를 사용하지만, 본 데이터는 Sequnece Data이므로 Conv1D를 사용\n",
    "* Sequence Data는 ResNet, Meta Data는 DNN\n",
    "* Concatenate 후, Reconstruction함 (Autoencoder). Loss Function은 mean_squared_error\n",
    "* Input -> Concatenate (Encoder)\n",
    "* Concatenate -> output (Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:38.817094Z",
     "start_time": "2020-06-26T06:55:38.811093Z"
    }
   },
   "outputs": [],
   "source": [
    "def res_unit(inputs, channels):\n",
    "    x = BatchNormalization()(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(channels, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(channels, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    added = Add()([inputs, x])\n",
    "    return added\n",
    "\n",
    "def res_unit_stride(inputs, channels):\n",
    "    x = BatchNormalization()(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(channels, kernel_size=3, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(channels, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    conv = Conv1D(channels, kernel_size=1, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    added = Add()([conv, x])\n",
    "    return added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:40.299428Z",
     "start_time": "2020-06-26T06:55:38.818094Z"
    }
   },
   "outputs": [],
   "source": [
    "# for seq_cols, Conv1D\n",
    "seq_in = Input(shape=(time_length,1))\n",
    "x = Conv1D(16, kernel_size=3, activation='relu', padding='same')(seq_in)\n",
    "x = MaxPooling1D(2, padding='same')(x)\n",
    "x =  res_unit(x, 16)\n",
    "x =  res_unit(x, 16)\n",
    "x =  res_unit(x, 16)\n",
    "x =  res_unit_stride(x, 32)\n",
    "x =  res_unit(x, 32)\n",
    "x =  res_unit(x, 32)\n",
    "x =  res_unit(x, 32)\n",
    "x =  res_unit_stride(x, 64)\n",
    "x =  res_unit(x, 64)\n",
    "x =  res_unit(x, 64)\n",
    "x =  res_unit(x, 64)\n",
    "x =  res_unit(x, 64)\n",
    "x =  res_unit_stride(x, 128)\n",
    "x =  res_unit(x, 128)\n",
    "x =  res_unit(x, 128)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "seq_out = x\n",
    "\n",
    "# for meta_cols\n",
    "meta_in = Input(shape=(len(meta_cols),))\n",
    "meta_out = Dense(20, kernel_initializer='he_normal', activation='elu')(meta_in)\n",
    "\n",
    "# Concat\n",
    "merges = Concatenate()([seq_out, meta_out])\n",
    "\n",
    "# decode\n",
    "seq_decode = Dense(time_length, kernel_initializer='he_normal')(merges)\n",
    "seq_decode = Reshape((time_length,1))(seq_decode)\n",
    "meta_decode = Dense(len(meta_cols), kernel_initializer='he_normal')(merges)\n",
    "\n",
    "model = Model(inputs=[seq_in, meta_in], outputs=[seq_decode, meta_decode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:40.310431Z",
     "start_time": "2020-06-26T06:55:40.300429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 44, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 44, 16)       64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 22, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 22, 16)       64          max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 22, 16)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 22, 16)       768         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 22, 16)       64          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 22, 16)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 22, 16)       768         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 22, 16)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 22, 16)       64          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 22, 16)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 22, 16)       768         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 22, 16)       64          conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 22, 16)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 22, 16)       768         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 22, 16)       0           add_1[0][0]                      \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 22, 16)       64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 22, 16)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 22, 16)       768         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 22, 16)       64          conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 22, 16)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 22, 16)       768         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 22, 16)       0           add_2[0][0]                      \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 22, 16)       64          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 22, 16)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 11, 32)       1536        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 11, 32)       128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 11, 32)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 11, 32)       512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 11, 32)       3072        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 11, 32)       0           conv1d_10[0][0]                  \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 11, 32)       128         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 11, 32)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 11, 32)       3072        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 11, 32)       128         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 11, 32)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 11, 32)       3072        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 11, 32)       0           add_4[0][0]                      \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 11, 32)       128         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 11, 32)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 11, 32)       3072        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 11, 32)       128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 11, 32)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 11, 32)       3072        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 11, 32)       0           add_5[0][0]                      \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 11, 32)       128         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 11, 32)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 11, 32)       3072        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 11, 32)       128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 11, 32)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 11, 32)       3072        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 11, 32)       0           add_6[0][0]                      \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 11, 32)       128         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 11, 32)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 6, 64)        6144        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 6, 64)        256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 6, 64)        0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 6, 64)        2048        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 6, 64)        12288       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 6, 64)        0           conv1d_19[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 6, 64)        256         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 6, 64)        0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 6, 64)        12288       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 6, 64)        256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 6, 64)        0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 6, 64)        12288       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 6, 64)        0           add_8[0][0]                      \n",
      "                                                                 conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 6, 64)        256         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 6, 64)        0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 6, 64)        12288       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 6, 64)        256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 6, 64)        0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 6, 64)        12288       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 6, 64)        0           add_9[0][0]                      \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 6, 64)        256         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 6, 64)        0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 6, 64)        12288       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 6, 64)        256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 6, 64)        0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 6, 64)        12288       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 6, 64)        0           add_10[0][0]                     \n",
      "                                                                 conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 6, 64)        256         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 6, 64)        0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 6, 64)        12288       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 6, 64)        256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 6, 64)        0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 6, 64)        12288       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 6, 64)        0           add_11[0][0]                     \n",
      "                                                                 conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 6, 64)        256         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 6, 64)        0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 3, 128)       24576       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 3, 128)       512         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 3, 128)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 3, 128)       8192        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 3, 128)       49152       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 3, 128)       0           conv1d_30[0][0]                  \n",
      "                                                                 conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 3, 128)       512         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 3, 128)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 3, 128)       49152       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 3, 128)       512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 3, 128)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 3, 128)       49152       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 3, 128)       0           add_13[0][0]                     \n",
      "                                                                 conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 3, 128)       512         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 3, 128)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 3, 128)       49152       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 3, 128)       512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 3, 128)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 3, 128)       49152       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 3, 128)       0           add_14[0][0]                     \n",
      "                                                                 conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 3, 128)       512         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 3, 128)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           520         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 148)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 44)           6556        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 44, 1)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 25)           3725        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 443,441\n",
      "Trainable params: 439,889\n",
      "Non-trainable params: 3,552\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:40.333436Z",
     "start_time": "2020-06-26T06:55:40.311431Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=['mean_squared_error', 'mean_squared_error'], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callback\n",
    "* Learning Rate를 잘 조절해준다는 Cyclic lr을 사용 (LRFinder Class)\n",
    "* epoch = 100, batch_size = 128, train_validation split 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:40.344439Z",
     "start_time": "2020-06-26T06:55:40.334436Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    '''\n",
    "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
    "    \n",
    "    # Usage\n",
    "        ```python\n",
    "            lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
    "            \n",
    "            lr_finder.plot_loss()\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
    "        \n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: https://arxiv.org/abs/1506.01186\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:55:40.348440Z",
     "start_time": "2020-06-26T06:55:40.345439Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T06:55:36.544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "705725/705725 [==============================] - 300s 426us/step - loss: 0.4510 - reshape_1_loss: 0.3307 - dense_3_loss: 0.1203\n",
      "Epoch 2/100\n",
      "705725/705725 [==============================] - 293s 415us/step - loss: 0.1697 - reshape_1_loss: 0.1536 - dense_3_loss: 0.0161\n",
      "Epoch 3/100\n",
      "705725/705725 [==============================] - 301s 427us/step - loss: 0.1217 - reshape_1_loss: 0.1061 - dense_3_loss: 0.0156\n",
      "Epoch 4/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0958 - reshape_1_loss: 0.0803 - dense_3_loss: 0.0155\n",
      "Epoch 5/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0801 - reshape_1_loss: 0.0647 - dense_3_loss: 0.0154\n",
      "Epoch 6/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0707 - reshape_1_loss: 0.0554 - dense_3_loss: 0.0153\n",
      "Epoch 7/100\n",
      "705725/705725 [==============================] - 295s 417us/step - loss: 0.0632 - reshape_1_loss: 0.0479 - dense_3_loss: 0.0153\n",
      "Epoch 8/100\n",
      "705725/705725 [==============================] - 297s 421us/step - loss: 0.0578 - reshape_1_loss: 0.0425 - dense_3_loss: 0.0153\n",
      "Epoch 9/100\n",
      "705725/705725 [==============================] - 297s 421us/step - loss: 0.0532 - reshape_1_loss: 0.0380 - dense_3_loss: 0.0152\n",
      "Epoch 10/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0494 - reshape_1_loss: 0.0343 - dense_3_loss: 0.0151\n",
      "Epoch 11/100\n",
      "705725/705725 [==============================] - 290s 411us/step - loss: 0.0473 - reshape_1_loss: 0.0322 - dense_3_loss: 0.0151\n",
      "Epoch 12/100\n",
      "705725/705725 [==============================] - 290s 410us/step - loss: 0.0424 - reshape_1_loss: 0.0273 - dense_3_loss: 0.0151\n",
      "Epoch 13/100\n",
      "705725/705725 [==============================] - 287s 406us/step - loss: 0.0418 - reshape_1_loss: 0.0267 - dense_3_loss: 0.0151\n",
      "Epoch 14/100\n",
      "705725/705725 [==============================] - 296s 419us/step - loss: 0.0397 - reshape_1_loss: 0.0247 - dense_3_loss: 0.0150\n",
      "Epoch 15/100\n",
      "705725/705725 [==============================] - 297s 421us/step - loss: 0.0385 - reshape_1_loss: 0.0236 - dense_3_loss: 0.0150\n",
      "Epoch 16/100\n",
      "705725/705725 [==============================] - 289s 409us/step - loss: 0.0371 - reshape_1_loss: 0.0220 - dense_3_loss: 0.0151\n",
      "Epoch 17/100\n",
      "705725/705725 [==============================] - 288s 408us/step - loss: 0.0360 - reshape_1_loss: 0.0209 - dense_3_loss: 0.0151\n",
      "Epoch 18/100\n",
      "705725/705725 [==============================] - 289s 410us/step - loss: 0.0344 - reshape_1_loss: 0.0194 - dense_3_loss: 0.0149\n",
      "Epoch 19/100\n",
      "705725/705725 [==============================] - 291s 413us/step - loss: 0.0340 - reshape_1_loss: 0.0190 - dense_3_loss: 0.0150\n",
      "Epoch 20/100\n",
      "705725/705725 [==============================] - 290s 411us/step - loss: 0.0333 - reshape_1_loss: 0.0184 - dense_3_loss: 0.0149\n",
      "Epoch 21/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0325 - reshape_1_loss: 0.0176 - dense_3_loss: 0.0149\n",
      "Epoch 22/100\n",
      "705725/705725 [==============================] - 291s 412us/step - loss: 0.0313 - reshape_1_loss: 0.0164 - dense_3_loss: 0.0150\n",
      "Epoch 23/100\n",
      "705725/705725 [==============================] - 292s 414us/step - loss: 0.0306 - reshape_1_loss: 0.0157 - dense_3_loss: 0.0149\n",
      "Epoch 24/100\n",
      "705725/705725 [==============================] - 290s 412us/step - loss: 0.0297 - reshape_1_loss: 0.0147 - dense_3_loss: 0.0150\n",
      "Epoch 25/100\n",
      "705725/705725 [==============================] - 291s 413us/step - loss: 0.0297 - reshape_1_loss: 0.0148 - dense_3_loss: 0.0149\n",
      "Epoch 26/100\n",
      "565760/705725 [=======================>......] - ETA: 57s - loss: 0.0294 - reshape_1_loss: 0.0145 - dense_3_loss: 0.0149"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_AE_seq, X_AE_seq,\n",
    "                    batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Save for Finetuing in Future\n",
    "* Modeling에서 Autoencoder의 중간층부터 학습에 사용하기 위해 저장한다.\n",
    "* encoder도 저장하고 model(=AE)도 저장하는데, 학습에는 encoder만 사용할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T06:55:36.545Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_keras_model(model, filename):\n",
    "    model_json = model.to_json()\n",
    "    with open('{}.json'.format(filename), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights('{}.h5'.format(filename))\n",
    "\n",
    "def load_keras_model(filename):\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open('{}.json'.format(filename), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights('{}.h5'.format(filename))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T06:55:36.546Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = './saved_models'\n",
    "filename = '{}/ResNetAE'.format(model_dir)\n",
    "\n",
    "save_keras_model(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T06:55:36.547Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = Model(inputs=[seq_in, meta_in], outputs=merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T06:55:36.549Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = '{}/ResNetAE_encoder'.format(model_dir)\n",
    "\n",
    "save_keras_model(encoder, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Folder\n",
    "     * './mart' 폴더에 (1)에서 저장한 Pickle 데이터가 들어 있어야 함\n",
    "     * './saved_models' 폴더에 (2)에서 저장한 Keras Model 데이터가 들어 있어야 함\n",
    "     * './ckpt' 폴더가 생성되어 있어야 함\n",
    "2.Package\n",
    "     * Python3.6\n",
    "     * pandas, numpy, os, time, pickle, tensorflow, keras, matplotlib, sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 불러오기\n",
    "* (1)에서 저장한 데이터 중, (X, y)를 불러온다.\n",
    "* Model의 Generalization 성능을 측정하기 위해 Train / Validation data split을 0.85 : 0.15 비율로 나눈다.\n",
    "* preproc_for_seq 함수를 통해, Sequence data를 학습하기 위한 전처리를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.865Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os, time, pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "def write_pickle(data, path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def read_pickle(path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "mart_path = './mart'\n",
    "time_length = 44\n",
    "timelength = time_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.870Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = read_pickle(mart_path, '44_develop_data')\n",
    "feature_cols = read_pickle(mart_path, '44_feature_cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.872Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_cols = feature_cols[:time_length]\n",
    "print(seq_cols)\n",
    "\n",
    "meta_cols = feature_cols[time_length:]\n",
    "print(meta_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.874Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.876Z"
    }
   },
   "outputs": [],
   "source": [
    "def preproc_for_seq(array):\n",
    "    \n",
    "    array_seq = []\n",
    "    \n",
    "    # seq_cols\n",
    "    array_seq.append(array[:,:timelength].reshape((array.shape[0], timelength, 1)))\n",
    "    \n",
    "    # meta_cols\n",
    "    array_seq.append(array[:,timelength:])\n",
    "    \n",
    "    return array_seq\n",
    "\n",
    "X_seq = preproc_for_seq(X)\n",
    "y_seq = y\n",
    "\n",
    "X_train_seq = preproc_for_seq(X_train)\n",
    "X_valid_seq = preproc_for_seq(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling - ResNet & DNN FineTuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 불러오기\n",
    "* (2)에서 저장했던 Encoder를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.878Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_keras_model(model, filename):\n",
    "    model_json = model.to_json()\n",
    "    with open('{}.json'.format(filename), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights('{}.h5'.format(filename))\n",
    "\n",
    "def load_keras_model(filename):\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open('{}.json'.format(filename), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights('{}.h5'.format(filename))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.879Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = './saved_models'\n",
    "filename = '{}/ResNetAE_encoder'.format(model_dir)\n",
    "encoder = load_keras_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Architecture\n",
    "* Encoder의 Output 위에 Dense Layer를 하나만 쌓는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.882Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_in, meta_in = encoder.inputs\n",
    "merges = encoder.outputs[0]\n",
    "y = Dense(1, kernel_initializer='he_normal', name='output')(merges)\n",
    "\n",
    "model = Model(inputs=[seq_in, meta_in], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.885Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss & Optimizer\n",
    "* Loss는 Competition 주최측에서 공지한 Evaluation Function * (-1)을 사용한다.\n",
    "* custom_loss가 그 역할을 수행한다.\n",
    "* optimizer는 Adam을 사용하고 Learning Rate는 0.001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.888Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    default_position = tf.less(y_true, y_pred)\n",
    "    profit_position = tf.logical_not(default_position)\n",
    "    pos = tf.reduce_sum(y_pred*tf.cast(profit_position, tf.float32)*13/365)\n",
    "    neg = tf.reduce_sum((y_true-y_pred)*tf.cast(default_position, tf.float32))\n",
    "    loss = -pos-neg\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.891Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=custom_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks\n",
    "* Learning Rate를 잘 조절해준다는 Cyclic lr을 사용 (LRFinder Class)\n",
    "* epoch = 100, batch_size = 128\n",
    "* ModelCheckpoint Callback을 사용해서, Validation data에서 역대 최저 Loss를 갱신할 때마다, 모델을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.893Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    '''\n",
    "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
    "    \n",
    "    # Usage\n",
    "        ```python\n",
    "            lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
    "            \n",
    "            lr_finder.plot_loss()\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
    "        \n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: https://arxiv.org/abs/1506.01186\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.896Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 128\n",
    "epoch_size = len(X_train)\n",
    "\n",
    "lr_finder = LRFinder(min_lr=1e-5, \n",
    "                     max_lr=1e-3, \n",
    "                     steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                     epochs=epochs)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
    "\n",
    "ckpt_dir = './ckpt'\n",
    "ckpt_path = ckpt_dir + '/ResNetFinetuning_{epoch:02d}_valloss{val_loss:.2f}.hdf5'\n",
    "ckpt = keras.callbacks.ModelCheckpoint(ckpt_path, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "#callbacks = [lr_finder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.898Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_seq, y_train,\n",
    "                    batch_size=batch_size, epochs=epochs,\n",
    "                    callbacks=[ckpt, lr_finder],\n",
    "                    validation_data=(X_valid_seq, y_valid), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Loading\n",
    "* 위 Training Log를 봤을 때, 국소적 역대 최고 Validation Loss를 기록한 것은, epoch 37, epoch 76, epoch 91, epoch 97 네 개이다\n",
    "* 아주 역대 최고는 epoch 97이지만, 76 ~ 91, 36 ~76 사이에 Validation Loss 하락 공백이 두 번 있었기 때문에, Overfitting을 의심하지 않을 수 없다.\n",
    "* 결론적으로, Competition에 Submit을 했을 때, 아래와 같은 Competition 점수를 얻었다\n",
    "    * epoch 37 = 53.8752점\n",
    "    * epoch 76 = 52.48691점 (3등 달성)\n",
    "    * epoch 91 = 66.65879점\n",
    "    * epoch 97 = 92점\n",
    "* 따라서, epoch 76에서의 ResNet + DNN Finetuning 모델은 training Loss 1.15, validation Loss 1.07, Test Score 52.48691의 우수한 성적을 내는 Overfitting이 되지 않으면서 우수한 Generalization 성능을 내는 우수한 단일 모델이라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.900Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights(ckpt_dir +'/ResNetFinetuning_76_valloss1.07.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.903Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_keras_model(model, filename):\n",
    "    model_json = model.to_json()\n",
    "    with open('{}.json'.format(filename), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights('{}.h5'.format(filename))\n",
    "\n",
    "def load_keras_model(filename):\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open('{}.json'.format(filename), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights('{}.h5'.format(filename))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.907Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = './saved_models'\n",
    "filename = '{}/ResNetFinetuning'.format(model_dir)\n",
    "\n",
    "save_keras_model(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data, Validation Data에 대해 y를 Predict한다.\n",
    "* Test Data: Submit을 위해\n",
    "* Validation Data: Stacking 등을 위해 (하지 않았지만)\n",
    "* y는 Scaling이 되어 있는 상태이므로, inverse_transform을 수행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.910Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = read_pickle(mart_path, '{}_X_test'.format(timelength))\n",
    "y_scaler = read_pickle(mart_path, '{}_y_scaler'.format(timelength))\n",
    "\n",
    "X_test_seq = preproc_for_seq(X_test)\n",
    "\n",
    "y_pred = model.predict(X_test_seq)\n",
    "y_pred_valid = model.predict(X_valid_seq)\n",
    "\n",
    "y_pred = y_scaler.inverse_transform(y_pred)\n",
    "y_pred_valid = y_scaler.inverse_transform(y_pred_valid)\n",
    "y_true_valid = y_scaler.inverse_transform(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data에 대한 Prediction을 Submission 양식에 맞추어 만들어낸다.\n",
    "* (1) Wranling 파트에서 store_id를 중복을 피하기 위해 조작을 했으므로, 본래의 store_id를 복구하기 위해 test_data를 다시 불러와서 원래의 store_id 정보를 얻어낸다.\n",
    "* Test data에 대한 Prediction과 함께 조합하여, Submission 파일을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Meta 정보 지정\n",
    "data_path = \"./data/\"\n",
    "train_filename = \"train.csv\"\n",
    "test_filename = \"test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(data_path + train_filename)\n",
    "test_data = pd.read_csv(data_path + test_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.917Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'store_id': test_data['store_id'].unique(), 'total_sales': y_pred.reshape(len(y_pred))})\n",
    "submission.to_csv('submission_ResNetFinetuning76epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data에 대한 Prediction을 추후 Stacking / Blending 등을 위해 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.921Z"
    }
   },
   "outputs": [],
   "source": [
    "write_pickle(y_pred_valid, \".\", 'y_pred_valid_ResNetFinetuning76epoch')\n",
    "write_pickle(y_true_valid, \".\", 'y_true_valid_ResNetFinetuning76epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "* 대회 주최측에서 알려준 Evaluation metric을 Python Function으로 만들었다.\n",
    "* Validation에 Evaluation Function을 적용했을 때, 101732063을 얻었고, 이는 만점 대비 0.4692700349674595%의 성과이다.\n",
    "* 이는 내가 지금까지 Evaluation했을 때 결과 중 Top 5 안에 들면서, Submit했을 때도 높은 점수를 얻은 결과라 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.923Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred, verbose=True):\n",
    "    \n",
    "    assert len(y_true) == len(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(len(y_true),)\n",
    "    y_pred = y_pred.reshape(len(y_true),)\n",
    "    \n",
    "    default_position = y_true - y_pred &lt; 0\n",
    "    profit_position = ~default_position\n",
    "    pos = np.sum(y_pred[profit_position]*13/365)\n",
    "    neg = np.sum(y_true[default_position] - y_pred[default_position])\n",
    "    score = pos + neg\n",
    "    if verbose:\n",
    "        print(\"The positive score: \")\n",
    "        print(pos)\n",
    "        print(\"The negative score: \")\n",
    "        print(neg)\n",
    "        print(\"The total score: \")\n",
    "        print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.925Z"
    }
   },
   "outputs": [],
   "source": [
    "score = get_score(y_true_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.927Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_percentage_to_upperbd(y_true, y_pred, verbose=False):\n",
    "    upperbd_score = get_score(y_true, y_true, verbose=False)\n",
    "    score = get_score(y_true, y_pred, verbose=False)\n",
    "    percentage = score/upperbd_score\n",
    "    if verbose:\n",
    "        print(\"The upper bound score: \")\n",
    "        print(upperbd_score)\n",
    "        print(\"The obtained score: \")\n",
    "        print(score)\n",
    "        print(\"The percentage to upper bound: \")\n",
    "        print(percentage)\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T07:29:46.929Z"
    }
   },
   "outputs": [],
   "source": [
    "percentage = get_percentage_to_upperbd(y_true_valid, y_pred_valid, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
