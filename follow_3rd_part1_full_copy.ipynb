{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 기본 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "* data import를 할 때, train_data와 test_data의 store_id가 중복되므로, 중복을 피하고자 test_data의 store_id에 다음과 같은 처리를 한다\n",
    "* test_data.loc[:,'store_id'] = test_data.loc[:,'store_id'] + train_data.loc[:,'store_id'].max() + 1\n",
    "* 나중에 Submit할 때는 store_id를 원상복구 해서 Submit하도록 한다\n",
    "* 중복을 피하고자 하는 이유는, train_data와 test_data를 합쳐서 data로 놓고, EDA와 Data Preprocessing을 동시에 진행하고자 함이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:52:38.592982Z",
     "start_time": "2020-06-24T06:52:38.589982Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os, time, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:52:38.595983Z",
     "start_time": "2020-06-24T06:52:38.593982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Meta 정보 지정\n",
    "data_path = \"./data/\"\n",
    "train_filename = \"train.csv\"\n",
    "test_filename = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:52:40.991337Z",
     "start_time": "2020-06-24T06:52:38.596983Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(data_path + train_filename)\n",
    "test_data = pd.read_csv(data_path + test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:52:41.018343Z",
     "start_time": "2020-06-24T06:52:40.992339Z"
    }
   },
   "outputs": [],
   "source": [
    "# test.csv랑 train.csv의 store_id가 같아도 같은 상점인 것은 아님\n",
    "test_data.loc[:,'store_id'] = test_data.loc[:,'store_id'] + train_data.loc[:,'store_id'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:52:41.238520Z",
     "start_time": "2020-06-24T06:52:41.019345Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.concat([train_data, test_data], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequnce 길이 정하기\n",
    "* test data의 store_id의 유니크한 값의 최소값 최대값을 보니 각각 44, 32959였다.\n",
    "* LSTM, GRU 등의 장기기억유닛은 대체로 100~500이 좋다고 했다.\n",
    "* 그러나 나의 경우, LSTM이나 GRU보다는 Conv1D가 좋았다 (결과적으로)\n",
    "* 따라서, Sequence의 길이는 최소값인 44로 정해서 zero padding을 하지 않는 방향으로 정한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:52:41.251524Z",
     "start_time": "2020-06-24T06:52:41.239520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      200.000000\n",
       "mean      2366.960000\n",
       "std       4243.725649\n",
       "min         44.000000\n",
       "25%        392.750000\n",
       "50%        982.000000\n",
       "75%       2321.750000\n",
       "max      32959.000000\n",
       "Name: store_id, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['store_id'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:52:41.261525Z",
     "start_time": "2020-06-24T06:52:41.252523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.0\n"
     ]
    }
   ],
   "source": [
    "time_length = test_data['store_id'].value_counts().describe()['min']\n",
    "timelength = time_length\n",
    "print(timelength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type Converting\n",
    "* 우선, date 컬럼과 time 컬럼의 경우, 합쳐서 datetime 컬럼으로 만든다\n",
    "* 이를, pandas dataframe의 index로 사용할 경우, padnas timeseries의 기능을 사용할 수 있으니, 추후 index로 사용하리라고 마음 먹는다.\n",
    "* installments 컬럼의 경우, 결측값이 있는데 이는 일시불이라고 컬럼 설명에 나와 있다.\n",
    "* 따라서, installments 컬럼의 결측값은 0으로 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:52:42.748006Z",
     "start_time": "2020-06-24T06:52:41.262526Z"
    }
   },
   "outputs": [],
   "source": [
    "data['datetime'] = pd.DatetimeIndex(data['date']+' '+data['time'])\n",
    "\n",
    "# fillna\n",
    "data.loc[:,\"installments\"].fillna(0, inplace=True)\n",
    "\n",
    "# drop columns\n",
    "data.drop(['date', 'time'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Column Meta 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:52:42.752007Z",
     "start_time": "2020-06-24T06:52:42.749007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['card_id', 'days_of_week', 'holyday', 'amount', 'installments']\n"
     ]
    }
   ],
   "source": [
    "all_cols = list(data.columns)\n",
    "key_cols = ['store_id']\n",
    "index_cols = ['datetime']\n",
    "feature_cols = list(set(all_cols) - set(key_cols+index_cols))\n",
    "\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sequence 마트 Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store_image 만들기\n",
    "* store_id에 대응하는 store_image를 만든다\n",
    "* 하나의 store_image는 datetime을 index로 가지고, feature_cols = ['time', 'days_of_week', 'holyday', 'amount', 'card_id', 'date', 'installments'] 를 컬럼으로 가진다.\n",
    "* store_image는 sotre_id가 Key인 파이썬 딕셔너리이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:53:35.192663Z",
     "start_time": "2020-06-24T06:53:10.584074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "store_image = {}\n",
    "\n",
    "for store_id in data['store_id'].unique():\n",
    "    store_image[store_id] = pd.DataFrame(data.loc[data.store_id == store_id,feature_cols].values,\n",
    "                                         index=data.loc[data.store_id == store_id,'datetime'].values, columns=feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount Column에 대해서 Daily Aggregation하기\n",
    "* sotre_id에 대해서 day를 기준으로 aggregation한다.\n",
    "* pandas timeseries의 resample 메서드가 이를 수행한다.\n",
    "* Daily Aggregation한 결과는 daily_amount_dict에 저장하는데, key는 store_id이고 value는 day가 index이고 amount가 column인 pandas timeseries이다.\n",
    "* Y도 동시에 정의하는데, Y의 정의는 100일 동안의 미래의 가맹점의 매출(=amount의 합)이므로, rolling(100)을 수행했을 때 Missing이 발생하면 Y가 없으므로 dropna로 날린다.\n",
    "* Y는 column_amount_dict에 저장하는데, Key는 store_id이고 value는 y이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:55:13.843172Z",
     "start_time": "2020-06-24T06:55:13.839169Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_check(df, value=0.0):\n",
    "    df[df==False] = value\n",
    "    df[df.isna()] = value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:56:39.778371Z",
     "start_time": "2020-06-24T06:55:14.344284Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumsum_amount exception occured at 31\n",
      "cumsum_amount exception occured at 42\n",
      "cumsum_amount exception occured at 53\n",
      "cumsum_amount exception occured at 61\n",
      "cumsum_amount exception occured at 75\n",
      "cumsum_amount exception occured at 93\n",
      "cumsum_amount exception occured at 101\n",
      "cumsum_amount exception occured at 102\n",
      "cumsum_amount exception occured at 164\n",
      "cumsum_amount exception occured at 183\n",
      "cumsum_amount exception occured at 201\n",
      "cumsum_amount exception occured at 203\n",
      "cumsum_amount exception occured at 208\n",
      "cumsum_amount exception occured at 229\n",
      "cumsum_amount exception occured at 248\n",
      "cumsum_amount exception occured at 254\n",
      "cumsum_amount exception occured at 272\n",
      "cumsum_amount exception occured at 295\n",
      "cumsum_amount exception occured at 303\n",
      "cumsum_amount exception occured at 308\n",
      "cumsum_amount exception occured at 316\n",
      "cumsum_amount exception occured at 321\n",
      "cumsum_amount exception occured at 344\n",
      "cumsum_amount exception occured at 350\n",
      "cumsum_amount exception occured at 351\n",
      "cumsum_amount exception occured at 364\n",
      "cumsum_amount exception occured at 387\n",
      "cumsum_amount exception occured at 416\n",
      "cumsum_amount exception occured at 436\n",
      "cumsum_amount exception occured at 446\n",
      "cumsum_amount exception occured at 468\n",
      "cumsum_amount exception occured at 470\n",
      "cumsum_amount exception occured at 479\n",
      "cumsum_amount exception occured at 505\n",
      "cumsum_amount exception occured at 509\n",
      "cumsum_amount exception occured at 516\n",
      "cumsum_amount exception occured at 523\n",
      "cumsum_amount exception occured at 524\n",
      "cumsum_amount exception occured at 599\n",
      "cumsum_amount exception occured at 604\n",
      "cumsum_amount exception occured at 608\n",
      "cumsum_amount exception occured at 611\n",
      "cumsum_amount exception occured at 617\n",
      "cumsum_amount exception occured at 623\n",
      "cumsum_amount exception occured at 632\n",
      "cumsum_amount exception occured at 636\n",
      "cumsum_amount exception occured at 641\n",
      "cumsum_amount exception occured at 652\n",
      "cumsum_amount exception occured at 657\n",
      "cumsum_amount exception occured at 660\n",
      "cumsum_amount exception occured at 664\n",
      "cumsum_amount exception occured at 679\n",
      "cumsum_amount exception occured at 703\n",
      "cumsum_amount exception occured at 718\n",
      "cumsum_amount exception occured at 727\n",
      "cumsum_amount exception occured at 734\n",
      "cumsum_amount exception occured at 763\n",
      "cumsum_amount exception occured at 764\n",
      "cumsum_amount exception occured at 789\n",
      "cumsum_amount exception occured at 808\n",
      "cumsum_amount exception occured at 814\n",
      "cumsum_amount exception occured at 816\n",
      "cumsum_amount exception occured at 817\n",
      "cumsum_amount exception occured at 818\n",
      "cumsum_amount exception occured at 826\n",
      "cumsum_amount exception occured at 845\n",
      "cumsum_amount exception occured at 847\n",
      "cumsum_amount exception occured at 854\n",
      "cumsum_amount exception occured at 866\n",
      "cumsum_amount exception occured at 892\n",
      "cumsum_amount exception occured at 906\n",
      "cumsum_amount exception occured at 931\n",
      "cumsum_amount exception occured at 949\n",
      "cumsum_amount exception occured at 951\n",
      "cumsum_amount exception occured at 952\n",
      "cumsum_amount exception occured at 966\n",
      "cumsum_amount exception occured at 970\n",
      "cumsum_amount exception occured at 982\n",
      "cumsum_amount exception occured at 984\n",
      "cumsum_amount exception occured at 988\n",
      "cumsum_amount exception occured at 991\n",
      "cumsum_amount exception occured at 1005\n",
      "cumsum_amount exception occured at 1009\n",
      "cumsum_amount exception occured at 1014\n",
      "cumsum_amount exception occured at 1018\n",
      "cumsum_amount exception occured at 1020\n",
      "cumsum_amount exception occured at 1043\n",
      "cumsum_amount exception occured at 1056\n",
      "cumsum_amount exception occured at 1066\n",
      "cumsum_amount exception occured at 1091\n",
      "cumsum_amount exception occured at 1116\n",
      "cumsum_amount exception occured at 1122\n",
      "cumsum_amount exception occured at 1123\n",
      "cumsum_amount exception occured at 1129\n",
      "cumsum_amount exception occured at 1159\n",
      "cumsum_amount exception occured at 1189\n",
      "cumsum_amount exception occured at 1193\n",
      "cumsum_amount exception occured at 1263\n",
      "cumsum_amount exception occured at 1265\n",
      "cumsum_amount exception occured at 1275\n",
      "cumsum_amount exception occured at 1288\n",
      "cumsum_amount exception occured at 1292\n",
      "cumsum_amount exception occured at 1397\n",
      "cumsum_amount exception occured at 1398\n",
      "cumsum_amount exception occured at 1412\n",
      "cumsum_amount exception occured at 1439\n",
      "cumsum_amount exception occured at 1469\n",
      "cumsum_amount exception occured at 1498\n",
      "cumsum_amount exception occured at 1530\n",
      "cumsum_amount exception occured at 1564\n",
      "cumsum_amount exception occured at 1568\n",
      "cumsum_amount exception occured at 1572\n",
      "cumsum_amount exception occured at 1600\n",
      "cumsum_amount exception occured at 1603\n",
      "cumsum_amount exception occured at 1605\n",
      "cumsum_amount exception occured at 1610\n",
      "cumsum_amount exception occured at 1613\n",
      "cumsum_amount exception occured at 1623\n",
      "cumsum_amount exception occured at 1624\n",
      "cumsum_amount exception occured at 1630\n",
      "cumsum_amount exception occured at 1642\n",
      "cumsum_amount exception occured at 1648\n",
      "cumsum_amount exception occured at 1656\n",
      "cumsum_amount exception occured at 1671\n",
      "cumsum_amount exception occured at 1698\n",
      "cumsum_amount exception occured at 1720\n",
      "cumsum_amount exception occured at 1721\n",
      "cumsum_amount exception occured at 1729\n",
      "cumsum_amount exception occured at 1744\n",
      "cumsum_amount exception occured at 1787\n",
      "cumsum_amount exception occured at 1789\n",
      "cumsum_amount exception occured at 1832\n",
      "cumsum_amount exception occured at 1884\n",
      "cumsum_amount exception occured at 1908\n"
     ]
    }
   ],
   "source": [
    "daily_amount_dict = {}\n",
    "cumsum_amount_dict = {}\n",
    "daily_amount_array_list = []\n",
    "cumsum_amount_array_list = []\n",
    "\n",
    "for store_id in data['store_id'].unique():\n",
    "    # daily aggregation\n",
    "    daily_amount = store_image[store_id][\"amount\"].resample('D').sum()\n",
    "    daily_amount = df_check(daily_amount)\n",
    "    daily_amount_array_list.append(daily_amount.values.reshape(len(daily_amount),1))\n",
    "    daily_amount_dict[store_id] = daily_amount\n",
    "    \n",
    "    # Y: 각 상점의 마지막 매출 발생일 다음 날부터 100일 후까지 매출의 총합\n",
    "    try:\n",
    "        cumsum_amount = daily_amount.rolling(100).sum().dropna().shift(-100, freq='D')\n",
    "        cumsum_amount_array_list.append(cumsum_amount.values.reshape(len(cumsum_amount),1))\n",
    "        cumsum_amount_dict[store_id] = cumsum_amount\n",
    "    except:\n",
    "        print(\"cumsum_amount exception occured at {}\".format(store_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Amount Value와 Y Value를 StandardScaler로 표준화하기\n",
    "* 나중에 Deep Learning 방법론을 사용할건데, Deep Learning 방법론은 Scaling에 굉장히 민감하다. 따라서 표준화를 진행한다.\n",
    "* 앞서 정의했던, daily_amount_array_list와 cumsum_amount_array_list에 저장된 Value를 이용하여, Scaler를 학습시킨다.\n",
    "* Scaler는 일반적으로 사용되는 sklearn.preprocessing.StandardScaler를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:57:37.970506Z",
     "start_time": "2020-06-24T06:57:37.903492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "daily_amount_values = np.vstack(daily_amount_array_list)\n",
    "cumsum_amount_values = np.vstack(cumsum_amount_array_list)\n",
    "\n",
    "daily_amount_scaler = StandardScaler()\n",
    "cumsum_amount_scaler = StandardScaler()\n",
    "\n",
    "daily_amount_scaler.fit(daily_amount_values)\n",
    "cumsum_amount_scaler.fit(cumsum_amount_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling & time_length(=44) 만큼의 Sequence를 Feature로 Wranling한다\n",
    "* daily_amount_scaler를 이용해서 daily_amount를 표준화\n",
    "* time_length(=44) 동안의 Daily amount를 Columns으로서 사용\n",
    "* 만약, 최근 44일 동안을 고려했을 때, 결측이 있는 경우 0으로 채움 (Zero padding)\n",
    "* Test data에서 최소 44일 이상은 있는 것을 확인했으니, 43일이 지난 데이터가 있는 경우에만 Data로 인정\n",
    "* Y도 역시 cumsum_amount_scaler를 이용해서 cumsum_amount를 표준화 (100일 지난 경우에만 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:58:50.070526Z",
     "start_time": "2020-06-24T06:57:39.176779Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumsum_amount exception occured at 31\n",
      "cumsum_amount exception occured at 42\n",
      "cumsum_amount exception occured at 53\n",
      "cumsum_amount exception occured at 61\n",
      "cumsum_amount exception occured at 75\n",
      "cumsum_amount exception occured at 93\n",
      "cumsum_amount exception occured at 101\n",
      "cumsum_amount exception occured at 102\n",
      "cumsum_amount exception occured at 164\n",
      "cumsum_amount exception occured at 183\n",
      "cumsum_amount exception occured at 201\n",
      "cumsum_amount exception occured at 203\n",
      "cumsum_amount exception occured at 208\n",
      "cumsum_amount exception occured at 229\n",
      "cumsum_amount exception occured at 248\n",
      "cumsum_amount exception occured at 254\n",
      "cumsum_amount exception occured at 272\n",
      "cumsum_amount exception occured at 295\n",
      "cumsum_amount exception occured at 303\n",
      "cumsum_amount exception occured at 308\n",
      "cumsum_amount exception occured at 316\n",
      "cumsum_amount exception occured at 321\n",
      "cumsum_amount exception occured at 344\n",
      "cumsum_amount exception occured at 350\n",
      "cumsum_amount exception occured at 351\n",
      "cumsum_amount exception occured at 364\n",
      "cumsum_amount exception occured at 387\n",
      "cumsum_amount exception occured at 416\n",
      "cumsum_amount exception occured at 436\n",
      "cumsum_amount exception occured at 446\n",
      "cumsum_amount exception occured at 468\n",
      "cumsum_amount exception occured at 470\n",
      "cumsum_amount exception occured at 479\n",
      "cumsum_amount exception occured at 505\n",
      "cumsum_amount exception occured at 509\n",
      "cumsum_amount exception occured at 516\n",
      "cumsum_amount exception occured at 523\n",
      "cumsum_amount exception occured at 524\n",
      "cumsum_amount exception occured at 599\n",
      "cumsum_amount exception occured at 604\n",
      "cumsum_amount exception occured at 608\n",
      "cumsum_amount exception occured at 611\n",
      "cumsum_amount exception occured at 617\n",
      "cumsum_amount exception occured at 623\n",
      "cumsum_amount exception occured at 632\n",
      "cumsum_amount exception occured at 636\n",
      "cumsum_amount exception occured at 641\n",
      "cumsum_amount exception occured at 652\n",
      "cumsum_amount exception occured at 657\n",
      "cumsum_amount exception occured at 660\n",
      "cumsum_amount exception occured at 664\n",
      "cumsum_amount exception occured at 679\n",
      "cumsum_amount exception occured at 703\n",
      "cumsum_amount exception occured at 718\n",
      "cumsum_amount exception occured at 727\n",
      "cumsum_amount exception occured at 734\n",
      "cumsum_amount exception occured at 763\n",
      "cumsum_amount exception occured at 764\n",
      "cumsum_amount exception occured at 789\n",
      "cumsum_amount exception occured at 808\n",
      "cumsum_amount exception occured at 814\n",
      "cumsum_amount exception occured at 816\n",
      "cumsum_amount exception occured at 817\n",
      "cumsum_amount exception occured at 818\n",
      "cumsum_amount exception occured at 826\n",
      "cumsum_amount exception occured at 845\n",
      "cumsum_amount exception occured at 847\n",
      "cumsum_amount exception occured at 854\n",
      "cumsum_amount exception occured at 866\n",
      "cumsum_amount exception occured at 892\n",
      "cumsum_amount exception occured at 906\n",
      "cumsum_amount exception occured at 931\n",
      "cumsum_amount exception occured at 949\n",
      "cumsum_amount exception occured at 951\n",
      "cumsum_amount exception occured at 952\n",
      "cumsum_amount exception occured at 966\n",
      "cumsum_amount exception occured at 970\n",
      "cumsum_amount exception occured at 982\n",
      "cumsum_amount exception occured at 984\n",
      "cumsum_amount exception occured at 988\n",
      "cumsum_amount exception occured at 991\n",
      "cumsum_amount exception occured at 1005\n",
      "cumsum_amount exception occured at 1009\n",
      "cumsum_amount exception occured at 1014\n",
      "cumsum_amount exception occured at 1018\n",
      "cumsum_amount exception occured at 1020\n",
      "cumsum_amount exception occured at 1043\n",
      "cumsum_amount exception occured at 1056\n",
      "cumsum_amount exception occured at 1066\n",
      "cumsum_amount exception occured at 1091\n",
      "cumsum_amount exception occured at 1116\n",
      "cumsum_amount exception occured at 1122\n",
      "cumsum_amount exception occured at 1123\n",
      "cumsum_amount exception occured at 1129\n",
      "cumsum_amount exception occured at 1159\n",
      "cumsum_amount exception occured at 1189\n",
      "cumsum_amount exception occured at 1193\n",
      "cumsum_amount exception occured at 1263\n",
      "cumsum_amount exception occured at 1265\n",
      "cumsum_amount exception occured at 1275\n",
      "cumsum_amount exception occured at 1288\n",
      "cumsum_amount exception occured at 1292\n",
      "cumsum_amount exception occured at 1397\n",
      "cumsum_amount exception occured at 1398\n",
      "cumsum_amount exception occured at 1412\n",
      "cumsum_amount exception occured at 1439\n",
      "cumsum_amount exception occured at 1469\n",
      "cumsum_amount exception occured at 1498\n",
      "cumsum_amount exception occured at 1530\n",
      "cumsum_amount exception occured at 1564\n",
      "cumsum_amount exception occured at 1568\n",
      "cumsum_amount exception occured at 1572\n",
      "cumsum_amount exception occured at 1600\n",
      "cumsum_amount exception occured at 1603\n",
      "cumsum_amount exception occured at 1605\n",
      "cumsum_amount exception occured at 1610\n",
      "cumsum_amount exception occured at 1613\n",
      "cumsum_amount exception occured at 1623\n",
      "cumsum_amount exception occured at 1624\n",
      "cumsum_amount exception occured at 1630\n",
      "cumsum_amount exception occured at 1642\n",
      "cumsum_amount exception occured at 1648\n",
      "cumsum_amount exception occured at 1656\n",
      "cumsum_amount exception occured at 1671\n",
      "cumsum_amount exception occured at 1698\n",
      "cumsum_amount exception occured at 1720\n",
      "cumsum_amount exception occured at 1721\n",
      "cumsum_amount exception occured at 1729\n",
      "cumsum_amount exception occured at 1744\n",
      "cumsum_amount exception occured at 1787\n",
      "cumsum_amount exception occured at 1789\n",
      "cumsum_amount exception occured at 1832\n",
      "cumsum_amount exception occured at 1884\n",
      "cumsum_amount exception occured at 1908\n"
     ]
    }
   ],
   "source": [
    "from pandas.tseries.offsets import Day\n",
    "\n",
    "seq_cols = ['amount_'+str(lag) for lag in np.arange(time_length-1, 0, -1)]\n",
    "amounts_seq_dict = {}\n",
    "\n",
    "for store_id in data['store_id'].unique():\n",
    "    # Scaling\n",
    "    daily_amount = daily_amount_dict[store_id]\n",
    "    daily_amount_scaled = daily_amount_scaler.transform(daily_amount.values.reshape(len(daily_amount),1))\n",
    "    daily_amount_scaled = pd.Series(daily_amount_scaled.reshape(len(daily_amount),), index=daily_amount.index)\n",
    "    daily_amount_dict[store_id] = daily_amount_scaled\n",
    "    \n",
    "    # Sequence\n",
    "    lag_amounts = []\n",
    "    \n",
    "    # time_length 동안의 Sequence 고려\n",
    "    for lag in np.arange(time_length-1, 0, -1):\n",
    "        lag_amount = daily_amount_scaled.shift(lag, freq='D')\n",
    "        lag_amounts.append(pd.DataFrame(lag_amount.values, index=lag_amount.index, columns=['amount_'+str(lag)]))\n",
    "        \n",
    "    amounts_seq = pd.concat(lag_amounts+[daily_amount_scaled], axis='columns', join='outer')\n",
    "    amounts_seq.columns = seq_cols + ['amount_0']\n",
    "    \n",
    "    # zero padding\n",
    "    amounts_seq = amounts_seq.loc[daily_amount_scaled.index, :].fillna(0.0)\n",
    "    # 적어도 43일 지났을 때부터 볼 것\n",
    "    amounts_seq = amounts_seq.loc[daily_amount.index[0] + Day(43):,:]\n",
    "    amounts_seq_dict[store_id] = amounts_seq\n",
    "    \n",
    "    # Y: 각 상점의 마지막 매출 발생일 다음 날부터 100일 후까지 매출의 총합\n",
    "    try:\n",
    "        cumsum_amount = cumsum_amount_dict[store_id]\n",
    "        cumsum_amount_scaled = cumsum_amount_scaler.transform(cumsum_amount.values.reshape(len(cumsum_amount),1))\n",
    "        cumsum_amount_dict[store_id] = pd.Series(cumsum_amount_scaled.reshape(len(cumsum_amount),), index=cumsum_amount.index)\n",
    "    except:\n",
    "        print(\"cumsum_amount exception occured at {}\".format(store_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Non-Sequence(=Meta) 마트 Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta - day 관련\n",
    "* 중요한 정보는 현재 무슨 요일, 공휴일이냐가 아니라, 앞으로 100일 동안 공휴일이 얼마나 많이 들어있지? 등의 미래에 대한 정보 (Future information)\n",
    "* 따라서 먼저, Futore information에 대한 정보를 Aggregation하기 전에, 먼저 무엇이 중요한 정보인지 판단한다.\n",
    "* 주관적으로 생각했을 때, 중요한 정보는 아래와 같다.\n",
    "    * 공휴일인가? (holyday)\n",
    "    * 빨간 날인가? (빨간 날 = 일요일 + 공휴일)\n",
    "    * 공휴일 전날인가?\n",
    "    * 완전히 쉬는 날인가? (완전히 쉬는 날 = 토요일 + 일요일 + 공휴일)\n",
    "    * 마음 놓고 유흥을 즐길 수 있는 날인가? (유흥 = 금요일 + 토요일 + 공휴일 전날)\n",
    "    * 영업일인가? (근로자 영업일 = ~완전히 쉬는 날)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:59:05.269090Z",
     "start_time": "2020-06-24T06:59:05.241085Z"
    }
   },
   "outputs": [],
   "source": [
    "day_info_raw = pd.DataFrame(data.loc[:,[\"days_of_week\", \"holyday\"]].values,\n",
    "                            columns=[\"days_of_week\", \"holyday\"],\n",
    "                            index=data.loc[:,'datetime'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:59:06.706870Z",
     "start_time": "2020-06-24T06:59:05.543157Z"
    }
   },
   "outputs": [],
   "source": [
    "day_info = day_info_raw.resample('D').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:59:06.711871Z",
     "start_time": "2020-06-24T06:59:06.707871Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.get_dummies(day_info.index.weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:59:06.727888Z",
     "start_time": "2020-06-24T06:59:06.712871Z"
    }
   },
   "outputs": [],
   "source": [
    "weekday_cols = [\"week_\"+str(i) for i in range(7)]\n",
    "\n",
    "for i, col in enumerate(weekday_cols):\n",
    "    weekday_info = pd.get_dummies(day_info.index.weekday)\n",
    "    day_info[col] = weekday_info.iloc[:,i].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:59:06.732890Z",
     "start_time": "2020-06-24T06:59:06.728889Z"
    }
   },
   "outputs": [],
   "source": [
    "# 공휴일 전날\n",
    "day_info[\"ex-holyday\"] = day_info[\"holyday\"].shift(-1).fillna(0).astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:59:06.745892Z",
     "start_time": "2020-06-24T06:59:06.733888Z"
    }
   },
   "outputs": [],
   "source": [
    "# 빨간 날 = 일요일 + 공휴일\n",
    "day_info[\"red_day\"] = day_info.loc[:,'week_6'] +  day_info.loc[:,'holyday'] - day_info.loc[:,'week_6']*day_info.loc[:,'holyday']\n",
    "\n",
    "# 유흥 = 금요일 + 토요일 + 공휴일 전날\n",
    "day_info[\"rest_day\"] = day_info.loc[:,'week_4'] + day_info.loc[:,'week_5'] +  day_info.loc[:,'ex-holyday']\\\n",
    "- day_info.loc[:,'week_4']*day_info.loc[:,'week_5'] - day_info.loc[:,'week_4']*day_info.loc[:,'ex-holyday'] - day_info.loc[:,'week_5']*day_info.loc[:,'ex-holyday']\\\n",
    "+ day_info.loc[:,'week_4']*day_info.loc[:,'week_5']*day_info.loc[:,'ex-holyday']\n",
    "\n",
    "# 완전히 쉬는 날 = 토요일 + 일요일 + 공휴일\n",
    "day_info[\"rest_day\"] = day_info.loc[:,'week_5'] + day_info.loc[:,'week_6'] +  day_info.loc[:,'holyday']\\\n",
    "- day_info.loc[:,'week_5']*day_info.loc[:,'week_6'] - day_info.loc[:,'week_5']*day_info.loc[:,'holyday'] - day_info.loc[:,'week_6']*day_info.loc[:,'holyday']\\\n",
    "+ day_info.loc[:,'week_5']*day_info.loc[:,'week_6']*day_info.loc[:,'holyday']\n",
    "\n",
    "# 근로자 영업일 = ~완전히 쉬는 날"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:59:07.090970Z",
     "start_time": "2020-06-24T06:59:07.079968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days_of_week</th>\n",
       "      <th>holyday</th>\n",
       "      <th>week_0</th>\n",
       "      <th>week_1</th>\n",
       "      <th>week_2</th>\n",
       "      <th>week_3</th>\n",
       "      <th>week_4</th>\n",
       "      <th>week_5</th>\n",
       "      <th>week_6</th>\n",
       "      <th>ex-holyday</th>\n",
       "      <th>red_day</th>\n",
       "      <th>rest_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-08-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-08-03</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            days_of_week  holyday  week_0  week_1  week_2  week_3  week_4  \\\n",
       "2016-08-01             0        0       1       0       0       0       0   \n",
       "2016-08-02             1        0       0       1       0       0       0   \n",
       "2016-08-03             2        0       0       0       1       0       0   \n",
       "2016-08-04             3        0       0       0       0       1       0   \n",
       "2016-08-05             4        0       0       0       0       0       1   \n",
       "\n",
       "            week_5  week_6  ex-holyday  red_day  rest_day  \n",
       "2016-08-01       0       0           0        0         0  \n",
       "2016-08-02       0       0           0        0         0  \n",
       "2016-08-03       0       0           0        0         0  \n",
       "2016-08-04       0       0           0        0         0  \n",
       "2016-08-05       0       0           0        0         0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta -Future day infomation\n",
    "* 중요한 정보는 현재 무슨 요일, 공휴일이냐가 아니라, 앞으로 100일 동안 공휴일이 얼마나 많이 들어있지? 등의 미래에 대한 정보 (Future information)\n",
    "* 따라서 먼저, Futore information에 대한 정보를 Aggregation한다\n",
    "* 주관적으로 생각했을 때, 중요한 정보는 아래와 같다.\n",
    "    * 앞으로 100일 동안 예정된 예정된 월화수목금토일\n",
    "    * 앞으로 100일 동안 예정된 빨간날\n",
    "    * 앞으로 100일 동안 예정된 유흥날\n",
    "    * 앞으로 100일 동안 예정된 쉬는날\n",
    "*Aggregation한 후, StandardScaler로 표준화를 수행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:59:08.639318Z",
     "start_time": "2020-06-24T06:59:08.631317Z"
    }
   },
   "outputs": [],
   "source": [
    "# 앞으로 예정된 월화수목금토일 + 빨간날, 유흥날, 쉬는날 총 갯수\n",
    "future_day_cols = list(day_info.columns)[3:]\n",
    "\n",
    "future_day_info = day_info.loc[:,future_day_cols].rolling(100).sum().dropna().shift(-100, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:59:09.073416Z",
     "start_time": "2020-06-24T06:59:09.063415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_1</th>\n",
       "      <th>week_2</th>\n",
       "      <th>week_3</th>\n",
       "      <th>week_4</th>\n",
       "      <th>week_5</th>\n",
       "      <th>week_6</th>\n",
       "      <th>ex-holyday</th>\n",
       "      <th>red_day</th>\n",
       "      <th>rest_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-08-02</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-08-03</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            week_1  week_2  week_3  week_4  week_5  week_6  ex-holyday  \\\n",
       "2016-07-31    15.0    14.0    14.0    14.0    14.0    14.0         6.0   \n",
       "2016-08-01    15.0    15.0    14.0    14.0    14.0    14.0         6.0   \n",
       "2016-08-02    14.0    15.0    15.0    14.0    14.0    14.0         6.0   \n",
       "2016-08-03    14.0    14.0    15.0    15.0    14.0    14.0         6.0   \n",
       "2016-08-04    14.0    14.0    14.0    15.0    15.0    14.0         6.0   \n",
       "\n",
       "            red_day  rest_day  \n",
       "2016-07-31     19.0      33.0  \n",
       "2016-08-01     19.0      33.0  \n",
       "2016-08-02     19.0      33.0  \n",
       "2016-08-03     19.0      33.0  \n",
       "2016-08-04     19.0      34.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_day_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:59:34.483061Z",
     "start_time": "2020-06-24T06:59:34.479061Z"
    }
   },
   "outputs": [],
   "source": [
    "day_scaler = StandardScaler()\n",
    "future_day_info_scaled = day_scaler.fit_transform(future_day_info.values)\n",
    "future_day_info_scaled = pd.DataFrame(future_day_info_scaled, columns=future_day_info.columns,\n",
    "                                      index=future_day_info.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta - Installments\n",
    "* 카드 결제 시 일시납 또는 할부 몇개월을 한다는 것은 그 가맹점의 고유한 특징을 반영한다고 생각 (주관적)\n",
    "* 따라서, Installments 컬럼에 대해서는 Day와 반대로 오히려 과거 Hisotry에 주목한다.\n",
    "* 과거 총 카드 결제 중 일시납의 비율, 할부 X개월의 비율을 Feature로서 사용하기 위해, Aggregation을 수행한다.\n",
    "* Aggregation 후에는, 역시 StandardScaler로 표준화를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:59:51.016592Z",
     "start_time": "2020-06-24T06:59:50.991587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 18, 20, 22, 24, 36]\n"
     ]
    }
   ],
   "source": [
    "installments_list = list(np.sort(data[\"installments\"].unique().astype(int)))[1:]\n",
    "installments_cols = ['installments_'+str(i) for i in installments_list]\n",
    "print(installments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:20:41.425016Z",
     "start_time": "2020-06-24T06:59:51.220638Z"
    }
   },
   "outputs": [],
   "source": [
    "installment_dict = {}\n",
    "installment_array_list = []\n",
    "\n",
    "for store_id in data['store_id'].unique():\n",
    "    # daily aggregation\n",
    "    daily_amount = daily_amount_dict[store_id]\n",
    "    daily_df = pd.DataFrame(daily_amount.values, columns=[\"amount\"], index=daily_amount.index)\n",
    "    \n",
    "    for col in installments_cols:\n",
    "        daily_df[col] = np.zeros(len(daily_df))\n",
    "    \n",
    "    for timeindex in daily_df.index:\n",
    "        installment_image = store_image[store_id][\"installments\"]\n",
    "        installment_history = installment_image[:timeindex].value_counts() / len(installment_image[:timeindex])\n",
    "        for i in installment_history.index:\n",
    "            daily_df.loc[timeindex, 'installments_'+str(int(i))] = installment_history[i]\n",
    "    \n",
    "    installment_dict[store_id] = daily_df.loc[:,installments_cols]\n",
    "    installment_array_list.append(daily_df.loc[:,installments_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:20:41.753929Z",
     "start_time": "2020-06-24T07:20:41.426017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "installment_values = np.vstack(installment_array_list)\n",
    "installment_scaler = StandardScaler()\n",
    "installment_scaler.fit(installment_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:20:42.261410Z",
     "start_time": "2020-06-24T07:20:41.760929Z"
    }
   },
   "outputs": [],
   "source": [
    "for store_id in data['store_id'].unique():\n",
    "    installment_df = installment_dict[store_id]\n",
    "    installment_df.loc[:,:] = installment_scaler.transform(installment_df.values)\n",
    "    installment_dict[store_id] = installment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Join for AE (AutoEncoder)\n",
    "* 위 2, 3번에서 말았던 Sequence data와 Day data, installments data를 Join한다.\n",
    "* 이 때, AutoEncoder는 비지도 학습이므로, y(=cumsum_amount)는 필요 없다\n",
    "* Output: X_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:20:45.786381Z",
     "start_time": "2020-06-24T07:20:42.262411Z"
    }
   },
   "outputs": [],
   "source": [
    "# For AE\n",
    "X_AE_list = []\n",
    "\n",
    "for store_id in data['store_id'].unique():\n",
    "    amounts_seq = amounts_seq_dict[store_id]\n",
    "    installment = installment_dict[store_id]\n",
    "    X_seq_meta_df = pd.concat([amounts_seq, installment, future_day_info_scaled], axis='columns', join='inner')\n",
    "    X_AE_list.append(X_seq_meta_df.values)\n",
    "\n",
    "X_AE = np.vstack(X_AE_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Join for Modeling\n",
    "* 위 2, 3번에서 말았던 Sequence data와 Day data, installments data, y(=cumsum_amount)를 Join한다.\n",
    "* 이 때, 지도 학습이므로, y(=cumsum_amount)도 필요한데, y값은 100일 지난 후까지의 데이터가 있어야 존재하므로, 이를 먼저 try 문으로 있는지 확인한다\n",
    "* Output: X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:20:50.394420Z",
     "start_time": "2020-06-24T07:20:45.790374Z"
    }
   },
   "outputs": [],
   "source": [
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for store_id in data['store_id'].unique():\n",
    "    try:\n",
    "        cumsum_amount_scaled = cumsum_amount_dict[store_id]\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        amounts_seq = amounts_seq_dict[store_id]\n",
    "        installment = installment_dict[store_id]\n",
    "        index_intersection = cumsum_amount_scaled.index.intersection(amounts_seq.index)\n",
    "        \n",
    "        X_seq_df = amounts_seq.loc[index_intersection, :]\n",
    "        X_seq_meta_df = pd.concat([X_seq_df, installment, future_day_info_scaled], axis='columns', join='inner')\n",
    "        y_series = cumsum_amount_scaled[index_intersection]\n",
    "        \n",
    "        X_list.append(X_seq_meta_df.loc[index_intersection, :].values)\n",
    "        y_list.append(y_series.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:20:50.580462Z",
     "start_time": "2020-06-24T07:20:50.395413Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack(X_list)\n",
    "y = np.concatenate(y_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Meta Define\n",
    "* X의 Feature를 List로 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:20:50.585462Z",
     "start_time": "2020-06-24T07:20:50.581454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amount_43.0', 'amount_42.0', 'amount_41.0', 'amount_40.0', 'amount_39.0', 'amount_38.0', 'amount_37.0', 'amount_36.0', 'amount_35.0', 'amount_34.0', 'amount_33.0', 'amount_32.0', 'amount_31.0', 'amount_30.0', 'amount_29.0', 'amount_28.0', 'amount_27.0', 'amount_26.0', 'amount_25.0', 'amount_24.0', 'amount_23.0', 'amount_22.0', 'amount_21.0', 'amount_20.0', 'amount_19.0', 'amount_18.0', 'amount_17.0', 'amount_16.0', 'amount_15.0', 'amount_14.0', 'amount_13.0', 'amount_12.0', 'amount_11.0', 'amount_10.0', 'amount_9.0', 'amount_8.0', 'amount_7.0', 'amount_6.0', 'amount_5.0', 'amount_4.0', 'amount_3.0', 'amount_2.0', 'amount_1.0', 'amount_0', 'installments_2', 'installments_3', 'installments_4', 'installments_5', 'installments_6', 'installments_7', 'installments_8', 'installments_9', 'installments_10', 'installments_12', 'installments_15', 'installments_18', 'installments_20', 'installments_22', 'installments_24', 'installments_36', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'ex-holyday', 'red_day', 'rest_day']\n"
     ]
    }
   ],
   "source": [
    "feature_cols = list(X_seq_meta_df.columns)\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data\n",
    "* test data는 AE와 비슷하게 y가 필요 없다.\n",
    "* 하지만, 가장 최신의 데이터를 기준으로 미래 100을 예측하는 문제이므로, datetime 기준 가장 마지막에 있는 데이터를 사용한다\n",
    "* 즉, pandas timeseries의 가장 마지막 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:20:50.863526Z",
     "start_time": "2020-06-24T07:20:50.586454Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_list = []\n",
    "\n",
    "for store_id in test_data['store_id'].unique():\n",
    "    amounts_seq = amounts_seq_dict[store_id]\n",
    "    installment = installment_dict[store_id]\n",
    "    X_seq_df = amounts_seq.iloc[[-1], :]\n",
    "    X_seq_meta_df = pd.concat([X_seq_df, installment, future_day_info_scaled], axis='columns', join='inner')\n",
    "    X_test_list.append(X_seq_meta_df.iloc[[-1], :].values)\n",
    "\n",
    "X_test = np.vstack(X_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Save\n",
    "* Data는 Python Pickle 파일로 저장한다.\n",
    "* 저장하는 파일 목록은 다음과 같다.\n",
    "    * (X, y) : Modeling Data\n",
    "    * feautre_cols : Column Meta\n",
    "    * X_test : Data for Scoring\n",
    "    * columns_amount_scaler: Need for converting original value for y\n",
    "    * X_AE : Data for Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:37:36.119441Z",
     "start_time": "2020-06-24T07:37:36.115441Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_pickle(data, path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def read_pickle(path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "mart_path = './mart/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:37:36.730579Z",
     "start_time": "2020-06-24T07:37:36.727579Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(mart_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T07:37:42.451869Z",
     "start_time": "2020-06-24T07:37:37.128669Z"
    }
   },
   "outputs": [],
   "source": [
    "write_pickle((X, y), mart_path, '44_develop_data')\n",
    "write_pickle(feature_cols, mart_path, '44_feature_cols')\n",
    "write_pickle(X_test, mart_path, '44_X_test')\n",
    "write_pickle(cumsum_amount_scaler, mart_path, '44_y_scaler')\n",
    "write_pickle(X_AE, mart_path, '44_X_AE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
