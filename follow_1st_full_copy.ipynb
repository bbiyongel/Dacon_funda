{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing for data cleaning 어떤 스토어들은 return 때문에 하루 매출 자체가 음수가 되는 경우도 있음.\n",
    "\n",
    "음수들은 Variance를 증대시키므로 prediction interval(http://bit.ly/2DnkoFD)\n",
    "\n",
    "이 커지게 되어 prediction에 악영향을 줄 수 있음.\n",
    "\n",
    "이 음수들을 제거하기 위해 카드 아이디를 이용하여 같은 카드의 negative transaction의 시간보다 작으면서 가장 최근 transaction 중\n",
    "\n",
    "1) 절대값이 같은 양수 가격의 transaction 을 제거 혹은\n",
    "\n",
    "2) 절대값이 더 큰 양수 가격의 transaction 에서 음수 transaction의 가격을 빼는 방법으로 음수들을 제거.\n",
    "\n",
    "만약 해당 카드 아이디가 그 전에 존재하지 않거나 1, 2에 해당하는 경우가 없을 경우 해당 환불의 실제 구매는 데이터의 시작 시간보다 과거이므로 데이터에서 제외 (line 32 - 66).\n",
    "\n",
    "business hour가 0시에 꼭 끝나지 않음으로 스토어에 따라 시간을 조정하여 하루를 1 business day로 만들면 좋음. business hour가 각 스토어마다 다르고, 스토어의 숫자가 많기도 하고 downsampling (크리스알본 : http://bit.ly/37A5l95 ) 하는 기간이 길면 이 부분은 어느정도 해결이 되므로 이 부분은 하지 않기로 결정.\n",
    "\n",
    "판매금액과 로그를 씌운 판매금액이 완전히는 아니지만 어느정도는 정규분포에 비슷함을 QQ plot으로 확인. outlier들의 영향을 줄이기 위해 로그를 씌운 판매금액을 종속변수로 사용하기로 결정.\n",
    "\n",
    "log를 씌운 후 값이 null이 되거나 무한이 되면 데이터셋에서 제거.\n",
    "\n",
    "null/무한대인 값들이 몇개인지 세어 몇 퍼센트의 샘플이 null/무한대인지 계산 후 최종 prediction에서 (1 - probability of no sales) 값으로 예측값을 곱하여 매출 예상에서 판매가 없을 확률만큼 discount함 (line 301).\n",
    "\n",
    "PACF (편자기상관함수 http://bit.ly/2OeRail)\n",
    "\n",
    "를 통해 AR(0), AR(1), AR(2)를 확인. 하지만 AIC를 통해 ARIMA모델에 사용될 pqr 값들을 찾을 때 0~2의 값들을 통해 구한 pqr보다 0~1 사이로 찾은 pqr이 더 정확한 예측을 했기 때문에 pqr의 범위를 0~1로 설정 (line 141-160)\n",
    "\n",
    "스토어마다 데이터 기간이 다름. 따라서 스토어 마다 다른 리샘플링 기간을 줄 필요가 있음.\n",
    "\n",
    "샘플이 충분하다면 최대한의 기간을 한 묶음으로 묶어 최소의 로 필요한 기간만큼 판매예측 하는 것이 더 정확한 예측 가능할 것이라 판단.\n",
    "\n",
    "요일에 따른 seasonality가 있으므로 최소 1주 단위로 묶어 seasonality를 없앨 필요가 있음.\n",
    "\n",
    "매년 있는 seasonality 또한 있으므로 365.25를 나눠서 잘 떨어지는 숫자로 정할 필요가 있음. 365.25를 나누면 7 일경우 52.18, 14일경우 26.09, 17.39, 28의 경우 13.04. 잘 나누어 떨어지는 숫자인 28, 14, 7일 단위로로 downsampling 하여, 최소 샘플의 숫자를 넘지 못하는 경우 점점 적은 기간으로 downsampling 함 (line 309-316).\n",
    "\n",
    "리샘플링 할 때 28 일씩 할 경우 일별로 묶인 샘플의 숫자로 28로 나누어 떨어지지 않으면 마지막 bin 안에 들어있는 샘플의 숫자가 다른 묶음안의 숫자보다 작음.\n",
    "\n",
    "각각의 bin 에 해당하는 기간이 같아야하고, 최근의 데이터가 먼 과거의 데이터보다 더 중요하기 때문에 28일로 나누어 떨어지지 않는 경우 앞의 남는 샘플들을 제거. 14일 단위의 bin으로 묶을 때도 같은 방법을 사용 (line 267).\n",
    "\n",
    "ARIMA모델에 사용되는 pqr 값을을 찾을때는 overfitting을 방지하기 위해 AIC 값을 이용해서 찾게 만듬 (parsimony) (line 145 - 160).\n",
    "\n",
    "산식 E(return) == 0.13 (100 + x) / 365 p = alpha p E(loss) == 1 (1-p) == beta (1-p) E(total return) == E(return) + E(loss) finding optimal p: alpha p == beta (1 - p) alpha p - beta p == beta p == beta / (alpha - beta) p == optimal p LN 세일즈가 표준분포에 흡사하세 퍼져있기 때문에 optimal p를 사용하여 optimal Z score를 계산. 이 Z score와 STD를 사용하여 확률적으로 점수계산공식에 가장 이상적인 예측값을 계산. (line 279 - 288) Optimization process: 28일, 14일, 7일씩 downsampling하여 예측하면 향후 84일 (28 3) 혹은 98일 (14 7 or 7 14) 동안의 매출을 예측 가능. 위의 E(return)안의 x값을 늘려 optimal p를 조절하는 방식으로 예측에서 빠진 기간동안의 매출을 채우기로 결정. 28일씩 리샘플링한 경우는 (84일 예측) x==22.8, 14일씩 리샘플링한 경우는 (98일 예측) x == 30 (30 이상은 test 해보지 않음)으로 했을 경우 좋은 결과가 나왔음 (Score == 49.42492). 최소의 샘플 숫자는 4, 5, 6, 8 중 6의 결과가 가장 좋아 6으로 결정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T06:06:30.556376Z",
     "start_time": "2020-06-24T06:00:09.468004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.38\n",
      "0.9580957780787686\n",
      "1.7290036387221377\n",
      "py_4arima_pos_sep_0.9581-28_no_sales_prob&no mean6&min_period 6_pdq2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import math\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import scipy.stats as st\n",
    "from datetime import datetime, timedelta\n",
    "warnings.filterwarnings(\"ignore\")  # specify to ignore warning messages\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "path = './data/'\n",
    "\n",
    "test = pd.read_csv(path+'test.csv')\n",
    "submission = pd.read_csv(path+'submission.csv')\n",
    "\n",
    "df_copy = test.copy()\n",
    "df_copy.date = pd.to_datetime(df_copy.date)\n",
    "\n",
    "# df_copy.date = pd.to_datetime(df_copy.date + \" \" + df_copy.time, format='%d/%m/%y %H:%M:%S')\n",
    "\n",
    "df_copy.date = pd.to_datetime(df_copy.date.astype(str) + \" \" + df_copy.time, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Remove negative values from the data set.\n",
    "\n",
    "def reduce_noise_by_removing_neg_vals(df_copy):\n",
    "    df_pos = df_copy[df_copy.amount > 0]\n",
    "    df_neg = df_copy[df_copy.amount < 0]\n",
    "\n",
    "    start = datetime.now()\n",
    "\n",
    "    for nega_i in df_neg.to_records()[:]:\n",
    "        store_i = nega_i[1]\n",
    "        date_i = nega_i[2]\n",
    "        card_i = nega_i[4]\n",
    "        amt_i = nega_i[5]\n",
    "        row_i = df_pos[df_pos.store_id == store_i]\n",
    "        row_i = row_i[row_i.card_id == card_i]\n",
    "        row_i = row_i[row_i.amount >= abs(amt_i)]\n",
    "        row_i = row_i[row_i.date <= date_i]\n",
    "        if len(row_i[row_i.amount == abs(amt_i)]) > 0:\n",
    "            row_i = row_i[row_i.amount == abs(amt_i)]\n",
    "            matched_row = row_i[row_i.date == max(row_i.date)]\n",
    "            # df_pos.loc[matched_row.index, 'amount'] = 0\n",
    "            df_pos = df_pos.loc[~df_pos.index.isin(matched_row.index), :]\n",
    "        elif len(row_i[row_i.amount > abs(amt_i)]) > 0:\n",
    "            matched_row = row_i[row_i.date == max(row_i.date)]\n",
    "            df_pos.loc[matched_row.index, 'amount'] = matched_row.amount + amt_i\n",
    "        # else:\n",
    "        #     pass\n",
    "            # no_match.append(nega_i)\n",
    "    end = datetime.now()\n",
    "    time_took = (end - start).seconds / 60\n",
    "\n",
    "    print(round(time_took, 2))\n",
    "    return df_pos\n",
    "\n",
    "df_pos = reduce_noise_by_removing_neg_vals(df_copy)\n",
    "\n",
    "def adf_test(y):\n",
    "    # perform Augmented Dickey Fuller test\n",
    "    print('Results of Augmented Dickey-Fuller test:')\n",
    "    dftest = adfuller(y, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['test statistic', 'p-value', '# of lags', '# of observations'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value ({})'.format(key)] = value\n",
    "    print(dfoutput)\n",
    "\n",
    "def ts_diagnostics(y, lags=None, title='', filename=''):\n",
    "    '''\n",
    "    Calculate acf, pacf, qq plot and Augmented Dickey Fuller test for a given time series\n",
    "    '''\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "\n",
    "    # weekly moving averages (5 day window because of workdays)\n",
    "    rolling_mean = pd.Series.rolling(y, window=2).mean()\n",
    "    rolling_std = pd.Series.rolling(y, window=2).std()\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 12))\n",
    "    layout = (3, 2)\n",
    "    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "    acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "    pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "    qq_ax = plt.subplot2grid(layout, (2, 0))\n",
    "    hist_ax = plt.subplot2grid(layout, (2, 1))\n",
    "\n",
    "    # time series plot\n",
    "    y.plot(ax=ts_ax)\n",
    "    rolling_mean.plot(ax=ts_ax, color='crimson')\n",
    "    rolling_std.plot(ax=ts_ax, color='darkslateblue')\n",
    "    plt.legend(loc='best')\n",
    "    ts_ax.set_title(title, fontsize=24)\n",
    "\n",
    "    # acf and pacf\n",
    "    plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n",
    "    plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n",
    "\n",
    "    # qq plot\n",
    "    sm.qqplot(y, line='s', ax=qq_ax)\n",
    "    qq_ax.set_title('QQ Plot')\n",
    "\n",
    "    # hist plot\n",
    "    y.plot(ax=hist_ax, kind='hist', bins=25)\n",
    "    hist_ax.set_title('Histogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # perform Augmented Dickey Fuller test\n",
    "    print('Results of Dickey-Fuller test:')\n",
    "    dftest = adfuller(y, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['test statistic', 'p-value', '# of lags', '# of observations'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' % key] = value\n",
    "    print(dfoutput)\n",
    "    return\n",
    "\n",
    "df = df_pos.copy()\n",
    "test_groupby_date_store = df.groupby(['date', 'store_id'])['amount', 'holyday'].sum()\n",
    "test_groupby_date_store = test_groupby_date_store.reset_index()\n",
    "\n",
    "test_groupby_date_store = test_groupby_date_store.set_index('date')\n",
    "store_list = test_groupby_date_store.store_id.unique()\n",
    "\n",
    "store_list.sort()\n",
    "\n",
    "\n",
    "def get_optimal_params(y):\n",
    "    # Define the p, d and q parameters to take any value between 0 and 1\n",
    "\n",
    "    param_dict = {}\n",
    "    for param in pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                            order=param,\n",
    "                                            )\n",
    "            results = mod.fit()\n",
    "            model = ARIMA(y, order=param)\n",
    "            results_ARIMA = model.fit(disp=-1)\n",
    "            results_ARIMA.summary()\n",
    "            param_dict[results.aic] = param\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    min_aic = min(param_dict.keys())\n",
    "    optimal_params = param_dict[min_aic]\n",
    "    return optimal_params\n",
    "\n",
    "\n",
    "sampling_p = 28\n",
    "mean_period = 2 * 3 #14 * 2*3\n",
    "\n",
    "predic_len = math.floor(100 / sampling_p)\n",
    "\n",
    "expected_return_pct_lending = 0.13 * (100 + 16 + 6.8) / 365\n",
    "expected_loss_pct_lending = 1.00\n",
    "optimal_prob = expected_loss_pct_lending / (expected_loss_pct_lending + expected_return_pct_lending)\n",
    "optimal_z_score = st.norm.ppf(optimal_prob)\n",
    "\n",
    "min_period = 6\n",
    "\n",
    "\n",
    "max_pdq = 2\n",
    "p = d = q = range(0, max_pdq)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "\n",
    "pdqs = dict()\n",
    "print(optimal_prob)\n",
    "print(optimal_z_score)\n",
    "output_file_name_fmt = 'py_4arima_pos_sep_{optimal_p}-{sampling_period}_no_sales_prob&no mean{mean_period}&min_period {min_period}_pdq{max_pdq}.csv'\n",
    "output_file_name = output_file_name_fmt.format(optimal_p=round(optimal_prob, 4),\n",
    "                                               sampling_period=sampling_p,\n",
    "                                               mean_period=mean_period,\n",
    "                                               min_period=min_period,\n",
    "                                               max_pdq=max_pdq)\n",
    "\n",
    "submission_copy = submission.copy()\n",
    "\n",
    "def arima_main(input_df, sampling_period_days, fcst_period):\n",
    "    input_df = input_df[len(input_df) % sampling_period_days:].resample(str(sampling_period_days) + 'D').sum()\n",
    "    prob_of_no_sales = len(input_df[(input_df.amount == 0) | (input_df.amount.isna())]) / len(input_df)\n",
    "    ts_log = np.log(input_df.amount)\n",
    "    ts_log = ts_log[~ts_log.isin([np.nan, np.inf, -np.inf])]\n",
    "\n",
    "    if len(ts_log) < min_period:\n",
    "        return None\n",
    "    if sampling_period_days >= 28:\n",
    "        expected_return_pct_lending = 0.13 * (100 + 16 + 6.8) / 365\n",
    "    elif sampling_period_days >= 14:\n",
    "        expected_return_pct_lending = 0.13 * (100 + 16 + 14) / 365\n",
    "    else:\n",
    "        expected_return_pct_lending = 0.13 * (100 + 16 + 6.8) / 365\n",
    "\n",
    "    expected_loss_pct_lending = 1.00\n",
    "    optimal_prob = expected_loss_pct_lending / (expected_loss_pct_lending + expected_return_pct_lending)\n",
    "    optimal_z_score = st.norm.ppf(optimal_prob)\n",
    "\n",
    "    optimal_params = get_optimal_params(ts_log)\n",
    "    pdqs[store_i] = optimal_params\n",
    "\n",
    "    model = ARIMA(ts_log, order=optimal_params)\n",
    "    results_ARIMA = model.fit(disp=-1)\n",
    "    fcst = results_ARIMA.forecast(fcst_period)\n",
    "\n",
    "    fcst_means = fcst[0]\n",
    "    fcst_stds = fcst[1]\n",
    "    fcst_i = fcst_means - (fcst_stds * optimal_z_score)\n",
    "    fcst_i = sum(map(lambda x: np.exp(x) if np.exp(x) > 0 else 0, fcst_i))\n",
    "    prediction_i = fcst_i * (1 - prob_of_no_sales)\n",
    "    return prediction_i\n",
    "\n",
    "for store_i in store_list[:]:\n",
    "    prediction_i = None\n",
    "    test_df = test_groupby_date_store[test_groupby_date_store.store_id == store_i]\n",
    "    test_df_daily = test_df.resample('D').sum()\n",
    "    prediction_i = arima_main(test_df_daily, sampling_period_days=28, fcst_period=3)\n",
    "    # if prediction_i is None:\n",
    "    #     prediction_i = arima_main(test_df_daily, sampling_period_days=21, fcst_period=4)\n",
    "    if prediction_i is None:\n",
    "        prediction_i = arima_main(test_df_daily, sampling_period_days=14, fcst_period=7)\n",
    "    if prediction_i is None:\n",
    "        prediction_i = arima_main(test_df_daily, sampling_period_days=7, fcst_period=12)\n",
    "    if prediction_i is None:\n",
    "        test_df = test_df_daily[len(test_df_daily) % 14:].resample('14D').sum()\n",
    "\n",
    "        prob_of_no_sales = len(test_df[(test_df.amount == 0) | (test_df.amount.isna())]) / len(test_df)\n",
    "        ts_log = ts_log[~ts_log.isin([np.nan, np.inf, -np.inf])]\n",
    "        ts_log_wkly = np.log(test_df.amount)\n",
    "\n",
    "        estimated_amt = np.exp(ts_log_wkly.mean() - ts_log_wkly.std() * optimal_z_score) * (1 - prob_of_no_sales)\n",
    "        prediction_i = estimated_amt * mean_period\n",
    "\n",
    "    submission_copy.loc[submission_copy['store_id'] == store_i, 'total_sales'] = prediction_i\n",
    "\n",
    "submission_copy.to_csv(output_file_name, index=False)\n",
    "\n",
    "print(output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
