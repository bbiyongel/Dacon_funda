{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Folder\n",
    "     * './mart' 폴더에 (1)에서 저장한 Pickle 데이터가 들어 있어야 함\n",
    "     * './saved_models' 폴더에 (2)에서 저장한 Keras Model 데이터가 들어 있어야 함\n",
    "     * './ckpt' 폴더가 생성되어 있어야 함\n",
    "2.Package\n",
    "     * Python3.6\n",
    "     * pandas, numpy, os, time, pickle, tensorflow, keras, matplotlib, sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 불러오기\n",
    "* (1)에서 저장한 데이터 중, (X, y)를 불러온다.\n",
    "* Model의 Generalization 성능을 측정하기 위해 Train / Validation data split을 0.85 : 0.15 비율로 나눈다.\n",
    "* preproc_for_seq 함수를 통해, Sequence data를 학습하기 위한 전처리를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os, time, pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "def write_pickle(data, path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def read_pickle(path, file_name):\n",
    "    with open(\"\".join([path, '/', file_name, '.pkl']), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "mart_path = './mart'\n",
    "time_length = 44\n",
    "timelength = time_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = read_pickle(mart_path, '44_develop_data')\n",
    "feature_cols = read_pickle(mart_path, '44_feature_cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_cols = feature_cols[:time_length]\n",
    "print(seq_cols)\n",
    "\n",
    "meta_cols = feature_cols[time_length:]\n",
    "print(meta_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_for_seq(array):\n",
    "    \n",
    "    array_seq = []\n",
    "    \n",
    "    # seq_cols\n",
    "    array_seq.append(array[:,:timelength].reshape((array.shape[0], timelength, 1)))\n",
    "    \n",
    "    # meta_cols\n",
    "    array_seq.append(array[:,timelength:])\n",
    "    \n",
    "    return array_seq\n",
    "\n",
    "X_seq = preproc_for_seq(X)\n",
    "y_seq = y\n",
    "\n",
    "X_train_seq = preproc_for_seq(X_train)\n",
    "X_valid_seq = preproc_for_seq(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling - ResNet & DNN FineTuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 불러오기\n",
    "* (2)에서 저장했던 Encoder를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keras_model(model, filename):\n",
    "    model_json = model.to_json()\n",
    "    with open('{}.json'.format(filename), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights('{}.h5'.format(filename))\n",
    "\n",
    "def load_keras_model(filename):\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open('{}.json'.format(filename), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights('{}.h5'.format(filename))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './saved_models'\n",
    "filename = '{}/ResNetAE_encoder'.format(model_dir)\n",
    "encoder = load_keras_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Architecture\n",
    "* Encoder의 Output 위에 Dense Layer를 하나만 쌓는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_in, meta_in = encoder.inputs\n",
    "merges = encoder.outputs[0]\n",
    "y = Dense(1, kernel_initializer='he_normal', name='output')(merges)\n",
    "\n",
    "model = Model(inputs=[seq_in, meta_in], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss & Optimizer\n",
    "* Loss는 Competition 주최측에서 공지한 Evaluation Function * (-1)을 사용한다.\n",
    "* custom_loss가 그 역할을 수행한다.\n",
    "* optimizer는 Adam을 사용하고 Learning Rate는 0.001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    default_position = tf.less(y_true, y_pred)\n",
    "    profit_position = tf.logical_not(default_position)\n",
    "    pos = tf.reduce_sum(y_pred*tf.cast(profit_position, tf.float32)*13/365)\n",
    "    neg = tf.reduce_sum((y_true-y_pred)*tf.cast(default_position, tf.float32))\n",
    "    loss = -pos-neg\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=custom_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks\n",
    "* Learning Rate를 잘 조절해준다는 Cyclic lr을 사용 (LRFinder Class)\n",
    "* epoch = 100, batch_size = 128\n",
    "* ModelCheckpoint Callback을 사용해서, Validation data에서 역대 최저 Loss를 갱신할 때마다, 모델을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    '''\n",
    "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
    "    \n",
    "    # Usage\n",
    "        ```python\n",
    "            lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
    "            \n",
    "            lr_finder.plot_loss()\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
    "        \n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: https://arxiv.org/abs/1506.01186\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 128\n",
    "epoch_size = len(X_train)\n",
    "\n",
    "lr_finder = LRFinder(min_lr=1e-5, \n",
    "                     max_lr=1e-3, \n",
    "                     steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                     epochs=epochs)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
    "\n",
    "ckpt_dir = './ckpt'\n",
    "ckpt_path = ckpt_dir + '/ResNetFinetuning_{epoch:02d}_valloss{val_loss:.2f}.hdf5'\n",
    "ckpt = keras.callbacks.ModelCheckpoint(ckpt_path, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "#callbacks = [lr_finder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_seq, y_train,\n",
    "                    batch_size=batch_size, epochs=epochs,\n",
    "                    callbacks=[ckpt, lr_finder],\n",
    "                    validation_data=(X_valid_seq, y_valid), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Loading\n",
    "* 위 Training Log를 봤을 때, 국소적 역대 최고 Validation Loss를 기록한 것은, epoch 37, epoch 76, epoch 91, epoch 97 네 개이다\n",
    "* 아주 역대 최고는 epoch 97이지만, 76 ~ 91, 36 ~76 사이에 Validation Loss 하락 공백이 두 번 있었기 때문에, Overfitting을 의심하지 않을 수 없다.\n",
    "* 결론적으로, Competition에 Submit을 했을 때, 아래와 같은 Competition 점수를 얻었다\n",
    "    * epoch 37 = 53.8752점\n",
    "    * epoch 76 = 52.48691점 (3등 달성)\n",
    "    * epoch 91 = 66.65879점\n",
    "    * epoch 97 = 92점\n",
    "* 따라서, epoch 76에서의 ResNet + DNN Finetuning 모델은 training Loss 1.15, validation Loss 1.07, Test Score 52.48691의 우수한 성적을 내는 Overfitting이 되지 않으면서 우수한 Generalization 성능을 내는 우수한 단일 모델이라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(ckpt_dir +'/ResNetFinetuning_76_valloss1.07.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keras_model(model, filename):\n",
    "    model_json = model.to_json()\n",
    "    with open('{}.json'.format(filename), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights('{}.h5'.format(filename))\n",
    "\n",
    "def load_keras_model(filename):\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open('{}.json'.format(filename), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights('{}.h5'.format(filename))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './saved_models'\n",
    "filename = '{}/ResNetFinetuning'.format(model_dir)\n",
    "\n",
    "save_keras_model(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data, Validation Data에 대해 y를 Predict한다.\n",
    "* Test Data: Submit을 위해\n",
    "* Validation Data: Stacking 등을 위해 (하지 않았지만)\n",
    "* y는 Scaling이 되어 있는 상태이므로, inverse_transform을 수행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = read_pickle(mart_path, '{}_X_test'.format(timelength))\n",
    "y_scaler = read_pickle(mart_path, '{}_y_scaler'.format(timelength))\n",
    "\n",
    "X_test_seq = preproc_for_seq(X_test)\n",
    "\n",
    "y_pred = model.predict(X_test_seq)\n",
    "y_pred_valid = model.predict(X_valid_seq)\n",
    "\n",
    "y_pred = y_scaler.inverse_transform(y_pred)\n",
    "y_pred_valid = y_scaler.inverse_transform(y_pred_valid)\n",
    "y_true_valid = y_scaler.inverse_transform(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data에 대한 Prediction을 Submission 양식에 맞추어 만들어낸다.\n",
    "* (1) Wranling 파트에서 store_id를 중복을 피하기 위해 조작을 했으므로, 본래의 store_id를 복구하기 위해 test_data를 다시 불러와서 원래의 store_id 정보를 얻어낸다.\n",
    "* Test data에 대한 Prediction과 함께 조합하여, Submission 파일을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Meta 정보 지정\n",
    "data_path = \"./data/\"\n",
    "train_filename = \"train.csv\"\n",
    "test_filename = \"test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(data_path + train_filename)\n",
    "test_data = pd.read_csv(data_path + test_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'store_id': test_data['store_id'].unique(), 'total_sales': y_pred.reshape(len(y_pred))})\n",
    "submission.to_csv('submission_ResNetFinetuning76epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data에 대한 Prediction을 추후 Stacking / Blending 등을 위해 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle(y_pred_valid, \".\", 'y_pred_valid_ResNetFinetuning76epoch')\n",
    "write_pickle(y_true_valid, \".\", 'y_true_valid_ResNetFinetuning76epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "* 대회 주최측에서 알려준 Evaluation metric을 Python Function으로 만들었다.\n",
    "* Validation에 Evaluation Function을 적용했을 때, 101732063을 얻었고, 이는 만점 대비 0.4692700349674595%의 성과이다.\n",
    "* 이는 내가 지금까지 Evaluation했을 때 결과 중 Top 5 안에 들면서, Submit했을 때도 높은 점수를 얻은 결과라 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred, verbose=True):\n",
    "    \n",
    "    assert len(y_true) == len(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(len(y_true),)\n",
    "    y_pred = y_pred.reshape(len(y_true),)\n",
    "    \n",
    "    default_position = y_true - y_pred &lt; 0\n",
    "    profit_position = ~default_position\n",
    "    pos = np.sum(y_pred[profit_position]*13/365)\n",
    "    neg = np.sum(y_true[default_position] - y_pred[default_position])\n",
    "    score = pos + neg\n",
    "    if verbose:\n",
    "        print(\"The positive score: \")\n",
    "        print(pos)\n",
    "        print(\"The negative score: \")\n",
    "        print(neg)\n",
    "        print(\"The total score: \")\n",
    "        print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = get_score(y_true_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_to_upperbd(y_true, y_pred, verbose=False):\n",
    "    upperbd_score = get_score(y_true, y_true, verbose=False)\n",
    "    score = get_score(y_true, y_pred, verbose=False)\n",
    "    percentage = score/upperbd_score\n",
    "    if verbose:\n",
    "        print(\"The upper bound score: \")\n",
    "        print(upperbd_score)\n",
    "        print(\"The obtained score: \")\n",
    "        print(score)\n",
    "        print(\"The percentage to upper bound: \")\n",
    "        print(percentage)\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = get_percentage_to_upperbd(y_true_valid, y_pred_valid, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
